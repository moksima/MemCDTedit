{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMex89dKCul1Bq5B5Q+o7CO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moksima/MemCDTedit/blob/main/GD%2BSAM%2BMat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOMg4pmy9M09",
        "outputId": "16701fd0-07dd-4ce4-8439-56a9be3cf3c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch==1.12.1+cu113 in /usr/local/lib/python3.10/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision==0.13.1+cu113 in /usr/local/lib/python3.10/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torchaudio==0.12.1 in /usr/local/lib/python3.10/dist-packages (0.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (11.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2024.8.30)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Collecting wget\n",
            "  Using cached wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=5dbe5e7dc8a731497a28d3ff1c1460fdad7e7c1634a1f0c961dcba2ceffe5281\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.11)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (0.40.2)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.8.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from supervision) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.10/dist-packages (from supervision) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from supervision) (6.0.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.13.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.13.1+cu113)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf) (8.5.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf) (4.3.6)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf) (2.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.10.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf) (3.20.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.4.21-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Collecting albucore==0.0.20 (from albumentations)\n",
            "  Downloading albucore-0.0.20-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.20->albumentations) (3.10.5)\n",
            "Collecting simsimd>=5.9.2 (from albucore==0.0.20->albumentations)\n",
            "  Downloading simsimd-5.9.9-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Downloading albumentations-1.4.21-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.9/227.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.20-py3-none-any.whl (12 kB)\n",
            "Downloading simsimd-5.9.9-cp310-cp310-manylinux_2_28_x86_64.whl (680 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.7/680.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\n",
            "  Attempting uninstall: albucore\n",
            "    Found existing installation: albucore 0.0.19\n",
            "    Uninstalling albucore-0.0.19:\n",
            "      Successfully uninstalled albucore-0.0.19\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.4.20\n",
            "    Uninstalling albumentations-1.4.20:\n",
            "      Successfully uninstalled albumentations-1.4.20\n",
            "Successfully installed albucore-0.0.20 albumentations-1.4.21 simsimd-5.9.9\n",
            "Collecting segment-anything\n",
            "  Downloading segment_anything-1.0-py3-none-any.whl.metadata (487 bytes)\n",
            "Downloading segment_anything-1.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install Correct PyTorch Version Compatible with Colab CUDA\n",
        "\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install transformers\n",
        "!pip install --upgrade pillow\n",
        "!pip install opencv-python\n",
        "!pip install wget\n",
        "!pip install addict numpy pycocotools supervision timm yapf albumentations tqdm\n",
        "!pip install -U albumentations\n",
        "!pip install segment-anything\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import Necessary Libraries\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "from google.colab import drive\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from groundingdino.models import build_model\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.utils import clean_state_dict\n",
        "from transformers import BertTokenizer\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings (optional)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
      ],
      "metadata": {
        "id": "_tLNk-9j9V2-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "f06f397b-92c0-4a60-b5cc-be7f8129083b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'groundingdino'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5812f116d3cc>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSLConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclean_state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'groundingdino'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oHzpsT3r9k27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Import google Drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "iaYZcGs89leu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Install Hugging Face Hub and Autenticiate\n",
        "\n",
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, hf_hub_download\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# Securely input your Hugging Face Token\n",
        "hf_token = getpass(\"Enter your Hugging Face API Token: \")\n",
        "login(token=hf_token)\n",
        "\n",
        "# Optionally, set the token as an environment variable\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n"
      ],
      "metadata": {
        "id": "tfKr6AK6QW81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New 4b: Verify CUDA Availability\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"Device count: {torch.cuda.device_count()}\")\n",
        "\n",
        "# New Cell: Check GPU Availability\n",
        "\n",
        "import torch\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n"
      ],
      "metadata": {
        "id": "Lm_nJsao0Ro3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define Custom Dataset for Fine-Tuning\n",
        "\n",
        "class MaterialSegmentationDataset(Dataset):\n",
        "    def __init__(self, list_file, images_dirs, masks_dirs, transforms=None,\n",
        "                 image_extensions=['.jpg', '.jpeg', '.png'],\n",
        "                 mask_extensions=['.npy', '.png', '.jpg', '.jpeg']):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            list_file (str): Path to the txt file containing filenames for the split.\n",
        "            images_dirs (list): List of directories containing images.\n",
        "            masks_dirs (list): List of directories containing masks.\n",
        "            transforms (albumentations.Compose, optional): Transformations to apply.\n",
        "            image_extensions (list): List of acceptable image file extensions.\n",
        "            mask_extensions (list): List of acceptable mask file extensions.\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "        self.images_dirs = images_dirs\n",
        "        self.masks_dirs = masks_dirs\n",
        "        self.image_extensions = image_extensions\n",
        "        self.mask_extensions = mask_extensions\n",
        "\n",
        "        # Read the list of filenames\n",
        "        with open(list_file, 'r') as f:\n",
        "            self.filenames = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        # Create a mapping from filename to image path\n",
        "        self.image_paths = []\n",
        "        missing_images = []\n",
        "        for fname in self.filenames:\n",
        "            found = False\n",
        "            for img_dir in self.images_dirs:\n",
        "                for ext in self.image_extensions:\n",
        "                    potential_path = os.path.join(img_dir, fname + ext)\n",
        "                    if os.path.isfile(potential_path):\n",
        "                        self.image_paths.append(potential_path)\n",
        "                        found = True\n",
        "                        break  # Stop searching extensions after finding the image\n",
        "                if found:\n",
        "                    break  # Stop searching directories after finding the image\n",
        "            if not found:\n",
        "                missing_images.append(fname)\n",
        "\n",
        "        if missing_images:\n",
        "            print(f\"Total missing images: {len(missing_images)}\")\n",
        "            for msg in missing_images[:5]:  # Print first 5 missing images\n",
        "                print(f\"Missing image: {msg}\")\n",
        "\n",
        "        # Create a mapping from filename to mask path\n",
        "        self.mask_paths = []\n",
        "        missing_masks = []\n",
        "        for fname in self.filenames:\n",
        "            found = False\n",
        "            base_name = os.path.splitext(fname)[0]\n",
        "            for mask_dir in self.masks_dirs:\n",
        "                for ext in self.mask_extensions:\n",
        "                    potential_mask_path = os.path.join(mask_dir, fname + ext)\n",
        "                    if os.path.isfile(potential_mask_path):\n",
        "                        self.mask_paths.append(potential_mask_path)\n",
        "                        found = True\n",
        "                        break  # Stop searching extensions after finding the mask\n",
        "                if found:\n",
        "                    break  # Stop searching directories after finding the mask\n",
        "            if not found:\n",
        "                missing_masks.append(fname)\n",
        "\n",
        "        if missing_masks:\n",
        "            print(f\"Total missing masks: {len(missing_masks)}\")\n",
        "            for msg in missing_masks[:5]:  # Print first 5 missing masks\n",
        "                print(f\"Missing mask: {msg}\")\n",
        "\n",
        "        # Filter out any image-mask pairs where either is missing\n",
        "        min_length = min(len(self.image_paths), len(self.mask_paths))\n",
        "        if len(self.image_paths) != len(self.mask_paths):\n",
        "            print(f\"Mismatched image and mask counts. Using first {min_length} pairs.\")\n",
        "            self.image_paths = self.image_paths[:min_length]\n",
        "            self.mask_paths = self.mask_paths[:min_length]\n",
        "\n",
        "        assert len(self.image_paths) == len(self.mask_paths), \"Mismatch between images and masks.\"\n",
        "\n",
        "        # Define your grayscale to class index mapping\n",
        "        self.grayscale_to_class = {\n",
        "            0: 0,     # Background\n",
        "            50: 1,    # Metal object\n",
        "            100: 2,   # Wooden door\n",
        "            150: 3,   # Concrete wall\n",
        "            200: 4,   # Glass window\n",
        "            250: 5,   # Soil\n",
        "            255: 6,   # Asphalt (or appropriate class)\n",
        "            # Add mappings for other classes as needed\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "\n",
        "        # Load mask\n",
        "        mask_path = self.mask_paths[idx]\n",
        "        if mask_path.endswith('.npy'):\n",
        "            mask = np.load(mask_path)\n",
        "            # Ensure mask is in the correct format (e.g., single channel)\n",
        "            if mask.ndim == 3:\n",
        "                mask = mask[:, :, 0]\n",
        "        elif mask_path.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
        "            mask = np.array(mask)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported mask format: {mask_path}\")\n",
        "\n",
        "        # Map mask values to class indices\n",
        "        mask = self.map_mask(mask, self.grayscale_to_class)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transforms:\n",
        "            augmented = self.transforms(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        # Ensure mask is of type torch.LongTensor\n",
        "        if isinstance(mask, np.ndarray):\n",
        "            mask = torch.from_numpy(mask).long()\n",
        "        elif isinstance(mask, torch.Tensor):\n",
        "            mask = mask.long()\n",
        "        else:\n",
        "            raise TypeError(f\"Unsupported mask type: {type(mask)}\")\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def map_mask(self, mask, mapping):\n",
        "        mapped_mask = np.full_like(mask, fill_value=-1, dtype=np.int64)  # Initialize with -1\n",
        "        for grayscale_value, class_index in mapping.items():\n",
        "            mapped_mask[mask == grayscale_value] = class_index\n",
        "\n",
        "        # Check for unmapped values\n",
        "        unmapped_pixels = (mapped_mask == -1)\n",
        "        if np.any(unmapped_pixels):\n",
        "            unique_unmapped_values = np.unique(mask[unmapped_pixels])\n",
        "            print(f\"Warning: Found unmapped grayscale values in mask: {unique_unmapped_values}\")\n",
        "            # Assign unmapped pixels to background class or any other class\n",
        "            mapped_mask[unmapped_pixels] = 0  # Assign to background class\n",
        "\n",
        "        return mapped_mask\n"
      ],
      "metadata": {
        "id": "jpydC21x9r4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Define Data Transformations\n",
        "\n",
        "# Define transformations for training\n",
        "train_transforms = A.Compose([\n",
        "    A.Resize(height=512, width=512),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "], additional_targets={'mask': 'mask'})\n",
        "\n",
        "# Define transformations for validation and testing\n",
        "val_transforms = A.Compose([\n",
        "    A.Resize(height=512, width=512),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "], additional_targets={'mask': 'mask'})\n"
      ],
      "metadata": {
        "id": "uMLUdqhh92Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Define Directory Paths and List Files\n",
        "\n",
        "# Define directories containing images\n",
        "images_dirs = [\n",
        "    \"/content/drive/MyDrive/multimodal_dataset/GT\",\n",
        "    \"/content/drive/MyDrive/multimodal_dataset/polL_color\"\n",
        "    # Add more image directories if applicable\n",
        "]\n",
        "\n",
        "# Define directories containing masks\n",
        "masks_dirs = [\n",
        "    \"/content/drive/MyDrive/multimodal_dataset/NIR_warped_mask\",\n",
        "    \"/content/drive/MyDrive/multimodal_dataset/polL_aolp_cos\",\n",
        "    \"/content/drive/MyDrive/multimodal_dataset/polL_aolp_sin\",\n",
        "    \"/content/drive/MyDrive/multimodal_dataset/polL_dolp\"\n",
        "    # Add more mask directories if applicable\n",
        "]\n",
        "\n",
        "# Paths to list files\n",
        "train_list = \"/content/drive/MyDrive/multimodal_dataset/list_folder/train.txt\"\n",
        "val_list = \"/content/drive/MyDrive/multimodal_dataset/list_folder/val.txt\"\n",
        "test_list = \"/content/drive/MyDrive/multimodal_dataset/list_folder/test.txt\"\n"
      ],
      "metadata": {
        "id": "9ntLlWdU97nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Create Dataset Instances\n",
        "\n",
        "# Use the transformations defined in Cell 6\n",
        "\n",
        "# Create Dataset instances\n",
        "train_dataset = MaterialSegmentationDataset(\n",
        "    list_file=train_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=train_transforms\n",
        ")\n",
        "\n",
        "val_dataset = MaterialSegmentationDataset(\n",
        "    list_file=val_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=val_transforms\n",
        ")\n",
        "\n",
        "test_dataset = MaterialSegmentationDataset(\n",
        "    list_file=test_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=val_transforms  # Typically, no augmentation for test\n",
        ")\n",
        "\n",
        "# Verify Dataset Lengths\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
        "\n",
        "# Visualize a Batch from the Training Loader\n",
        "def visualize_batch(images, masks, batch_size=4):\n",
        "    images = images.permute(0, 2, 3, 1).cpu().numpy()\n",
        "    images = (images * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "    images = np.clip(images, 0, 1)\n",
        "\n",
        "    masks = masks.cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    for i in range(batch_size):\n",
        "        plt.subplot(2, batch_size, i+1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(\"Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, batch_size, batch_size + i + 1)\n",
        "        plt.imshow(masks[i], cmap='gray')\n",
        "        plt.title(\"Mask\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get a batch from the training loader\n",
        "for images, masks in DataLoader(train_dataset, batch_size=4):\n",
        "    visualize_batch(images, masks, batch_size=4)\n",
        "    break  # Only visualize one batch\n"
      ],
      "metadata": {
        "id": "8czZ9IXA-ANP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Define DataLoaders\n",
        "\n",
        "batch_size = 8\n",
        "num_workers = 4  # Adjust based on your environment\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=False  # Temporarily set to False for debugging\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=False  # Temporarily set to False for debugging\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=False  # Temporarily set to False for debugging\n",
        ")\n"
      ],
      "metadata": {
        "id": "JCpIFcHB-FdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Verify Dataset and DataLoaders\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
        "\n",
        "# Function to visualize image and mask\n",
        "def visualize_batch(images, masks, batch_size=4):\n",
        "    images = images.permute(0, 2, 3, 1).cpu().numpy()\n",
        "    images = (images * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "    images = np.clip(images, 0, 1)\n",
        "\n",
        "    masks = masks.cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    for i in range(batch_size):\n",
        "        plt.subplot(2, batch_size, i+1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(\"Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, batch_size, batch_size + i + 1)\n",
        "        plt.imshow(masks[i], cmap='gray')\n",
        "        plt.title(\"Mask\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Check mask values function\n",
        "def check_mask_values(dataloader, num_classes):\n",
        "    invalid_values = False\n",
        "    for images, masks in dataloader:\n",
        "        masks_np = masks.cpu().numpy()\n",
        "        min_value = masks_np.min()\n",
        "        max_value = masks_np.max()\n",
        "        if min_value < 0 or max_value >= num_classes:\n",
        "            print(f\"Invalid mask values found. Min value: {min_value}, Max value: {max_value}\")\n",
        "            invalid_values = True\n",
        "            break\n",
        "    if not invalid_values:\n",
        "        print(\"All mask values are within the valid range.\")\n",
        "\n",
        "# Define number of classes (update based on your mapping)\n",
        "num_classes = 7  # Updated to match your grayscale_to_class mapping\n",
        "\n",
        "# Check mask values in training and validation loaders\n",
        "print(\"Checking mask values in training loader:\")\n",
        "check_mask_values(train_loader, num_classes)\n",
        "\n",
        "print(\"Checking mask values in validation loader:\")\n",
        "check_mask_values(val_loader, num_classes)\n",
        "\n",
        "# Get a batch from the training loader and visualize\n",
        "for images, masks in train_loader:\n",
        "    visualize_batch(images, masks, batch_size=4)\n",
        "    break  # Only visualize one batch\n"
      ],
      "metadata": {
        "id": "9RM9UcKK-JpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Define and Initialize the Segmentation Model\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Verify CUDA device\n",
        "if device.type == 'cuda':\n",
        "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Using DeepLabV3 with a ResNet backbone\n",
        "from torchvision import models\n",
        "model = models.segmentation.deeplabv3_resnet50(weights='DEFAULT')\n",
        "num_classes = 7  # Updated to match your mapping\n",
        "\n",
        "# Modify the classifier to match the number of classes\n",
        "# Wrap in try-except to catch any errors\n",
        "try:\n",
        "    model.classifier = models.segmentation.deeplabv3.DeepLabHead(2048, num_classes)\n",
        "    print(\"Model classifier modified successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error modifying model classifier: {e}\")\n",
        "\n",
        "# Move model to device\n",
        "try:\n",
        "    model = model.to(device)\n",
        "    print(\"Model moved to device successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error moving model to device: {e}\")\n"
      ],
      "metadata": {
        "id": "FQo6mOar-Olh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wz2YLdB3dfoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Install Grounding DINO via pip (Replace 'Z' with an appropriate cell number)\n",
        "\n",
        "!pip install git+https://github.com/IDEA-Research/GroundingDINO.git\n"
      ],
      "metadata": {
        "id": "nYt_uXFJdgOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13a: Verify Mask Values Are Within Valid Range\n",
        "\n",
        "def check_mask_values(dataloader, num_classes):\n",
        "    invalid_values = False\n",
        "    for images, masks in dataloader:\n",
        "        masks = masks.cpu().numpy()\n",
        "        min_value = masks.min()\n",
        "        max_value = masks\n"
      ],
      "metadata": {
        "id": "xb0GNs5ehAvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13b: Define Helper Functions to Load Models\n",
        "\n",
        "import torch\n",
        "from groundingdino.models import build_model\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.utils import clean_state_dict\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "def load_grounding_dino(config_path, checkpoint_path, device='cuda'):\n",
        "    \"\"\"\n",
        "    Load the Grounding DINO model from configuration and checkpoint files.\n",
        "    \"\"\"\n",
        "    # Load configuration\n",
        "    cfg = SLConfig.fromfile(config_path)\n",
        "\n",
        "    # Build model\n",
        "    model = build_model(cfg)\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
        "\n",
        "    # Move model to device and set to evaluation mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_sam(sam_checkpoint_path, model_type=\"vit_b\", device='cuda'):\n",
        "    \"\"\"\n",
        "    Load the SAM model and initialize the predictor.\n",
        "    \"\"\"\n",
        "    # Register and load SAM model\n",
        "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint_path)\n",
        "    sam.to(device)\n",
        "    sam_predictor = SamPredictor(sam)\n",
        "\n",
        "    return sam_predictor\n",
        "\n",
        "def load_image(image_path):\n",
        "    \"\"\"\n",
        "    Load an image from the specified path and preprocess it.\n",
        "    \"\"\"\n",
        "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                             std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "    image_tensor = preprocess(image_pil)\n",
        "\n",
        "    return image_pil, image_tensor\n",
        "\n",
        "def get_grounding_output_updated(model, image, caption, box_threshold, text_threshold, device='cuda'):\n",
        "    \"\"\"\n",
        "    Perform Grounding DINO inference to obtain bounding boxes and associated phrases.\n",
        "\n",
        "    Args:\n",
        "        model: The loaded Grounding DINO model.\n",
        "        image (torch.Tensor): Preprocessed image tensor of shape (C, H, W).\n",
        "        caption (str): Text prompt containing multiple categories separated by commas.\n",
        "        box_threshold (float): Confidence threshold to filter bounding boxes.\n",
        "        text_threshold (float): Confidence threshold for text recognition.\n",
        "        device (str): Device to perform computation ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        boxes_filt (torch.Tensor): Filtered bounding boxes with shape (num_boxes, 4).\n",
        "        pred_phrases (list): List of predicted phrases corresponding to each bounding box.\n",
        "        scores (torch.Tensor): Confidence scores for each bounding box.\n",
        "    \"\"\"\n",
        "    image = image.to(device)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image[None], text=[caption])  # Changed 'captions' to 'text'\n",
        "\n",
        "    # Debug: Print output keys\n",
        "    print(f\"Model outputs keys: {outputs.keys()}\")\n",
        "\n",
        "    # Check if 'pred_logits' and 'pred_boxes' are in outputs\n",
        "    if 'pred_logits' not in outputs or 'pred_boxes' not in outputs:\n",
        "        print(\"Model outputs do not contain 'pred_logits' or 'pred_boxes'.\")\n",
        "        return torch.empty((0, 4)).to(device), [], torch.empty((0,)).to(device)\n",
        "\n",
        "    logits = outputs[\"pred_logits\"].sigmoid()[0]  # Shape: (num_queries, vocab_size)\n",
        "    boxes = outputs[\"pred_boxes\"][0]              # Shape: (num_queries, 4)\n",
        "\n",
        "    # Filter boxes with confidence threshold\n",
        "    logits_max, _ = logits.max(dim=1)\n",
        "    keep = logits_max > box_threshold\n",
        "    if keep.sum() == 0:\n",
        "        print(\"No boxes above box_threshold.\")\n",
        "        return torch.empty((0, 4)).to(device), [], torch.empty((0,)).to(device)\n",
        "    logits_filt = logits[keep]\n",
        "    boxes_filt = boxes[keep]\n",
        "\n",
        "    # Tokenize caption\n",
        "    tokenized = tokenizer(caption, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Map logits to phrases\n",
        "    pred_phrases = []\n",
        "    scores = []\n",
        "    for logit in logits_filt:\n",
        "        # Find tokens with confidence above text_threshold\n",
        "        token_indices = (logit > text_threshold).nonzero(as_tuple=True)[0]\n",
        "        if len(token_indices) == 0:\n",
        "            continue\n",
        "        tokens = tokenized[\"input_ids\"][0][token_indices]\n",
        "        phrase = tokenizer.decode(tokens)\n",
        "        pred_phrases.append(phrase)\n",
        "        scores.append(logit[token_indices].mean().item())\n",
        "\n",
        "    if len(scores) == 0:\n",
        "        print(\"No scores above text_threshold.\")\n",
        "        return torch.empty((0, 4)).to(device), [], torch.empty((0,)).to(device)\n",
        "\n",
        "    boxes_filt = boxes_filt[:len(pred_phrases)]\n",
        "    scores = torch.tensor(scores).to(device)\n",
        "    return boxes_filt, pred_phrases, scores\n",
        "\n",
        "def segment_with_sam(image_pil, boxes, predictor):\n",
        "    \"\"\"\n",
        "    Perform segmentation using the SAM model based on bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        image_pil (PIL.Image.Image): The input image.\n",
        "        boxes (torch.Tensor): Bounding boxes detected by Grounding DINO.\n",
        "        predictor: The SAM predictor object.\n",
        "\n",
        "    Returns:\n",
        "        List of NumPy arrays representing segmentation masks.\n",
        "    \"\"\"\n",
        "    if boxes.numel() == 0:\n",
        "        print(\"No boxes provided for segmentation.\")\n",
        "        return []\n",
        "\n",
        "    image_np = np.array(image_pil)\n",
        "    predictor.set_image(image_np)\n",
        "    masks = []\n",
        "\n",
        "    transformed_boxes = predictor.transform.apply_boxes_torch(boxes, image_np.shape[:2])\n",
        "\n",
        "    for box in transformed_boxes:\n",
        "        # Perform prediction\n",
        "        masks_pred, _, _ = predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=box.unsqueeze(0).to(device),\n",
        "            multimask_output=False,\n",
        "        )\n",
        "\n",
        "        # Convert mask to NumPy array\n",
        "        mask_np = masks_pred[0].cpu().numpy()\n",
        "        masks.append(mask_np)\n",
        "\n",
        "    return masks\n",
        "\n",
        "# **Add confirmation print statements**\n",
        "print(\"Helper functions defined successfully.\")\n",
        "print(f\"Function load_grounding_dino: {load_grounding_dino}\")\n",
        "print(f\"Function load_sam: {load_sam}\")\n",
        "print(f\"Function load_image: {load_image}\")\n",
        "print(f\"Function get_grounding_output_updated: {get_grounding_output_updated}\")\n",
        "print(f\"Function segment_with_sam: {segment_with_sam}\")\n"
      ],
      "metadata": {
        "id": "I6anH0R2jG8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Update timm to Latest Version to Avoid FutureWarnings\n",
        "\n",
        "!pip install --upgrade timm\n"
      ],
      "metadata": {
        "id": "C10M45pFlAvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: Verify Helper Functions Defined Correctly\n",
        "\n",
        "try:\n",
        "    print(load_grounding_dino)\n",
        "    print(load_sam)\n",
        "    print(load_image)\n",
        "    print(get_grounding_output)\n",
        "    print(segment_with_sam)\n",
        "    print(\"All helper functions are defined correctly.\")\n",
        "except NameError as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "sscc9bx4lB2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Define Loss Function, Optimizer, and Scheduler\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "learning_rate = 1e-4\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
      ],
      "metadata": {
        "id": "f1jjQDwkjCFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 17: Download SAM Checkpoint (Re-execute if necessary)\n",
        "\n",
        "import os\n",
        "import wget\n",
        "\n",
        "sam_checkpoint_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
        "sam_checkpoint_dir = \"/content/segment-anything/checkpoints\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(sam_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Download the SAM checkpoint\n",
        "!wget -O {sam_checkpoint_dir}/sam_vit_b_01ec64.pth {sam_checkpoint_url}\n",
        "\n",
        "# Verify SAM Checkpoint\n",
        "sam_checkpoint_path = \"/content/segment-anything/checkpoints/sam_vit_b_01ec64.pth\"\n",
        "print(f\"SAM Checkpoint Exists: {os.path.exists(sam_checkpoint_path)}\")\n",
        "\n",
        "# If not, list the contents of the directory to see available files\n",
        "if not os.path.exists(sam_checkpoint_path):\n",
        "    print(\"Contents of the SAM Checkpoint Directory:\")\n",
        "    print(os.listdir(\"/content/segment-anything/checkpoints\"))\n"
      ],
      "metadata": {
        "id": "PTnK1ySfjA6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 18: Clone Grounding DINO Repository\n",
        "\n",
        "!pip install git+https://github.com/IDEA-Research/GroundingDINO.git\n",
        "\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n"
      ],
      "metadata": {
        "id": "rQZuuPQlUhhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 19: Verify Grounding DINO Checkpoint Path and Filename\n",
        "\n",
        "import os\n",
        "\n",
        "grounding_dino_checkpoint_dir = \"/content/drive/MyDrive/GroundingDINO/checkpoint\"\n",
        "grounding_dino_checkpoint = \"/content/drive/MyDrive/GroundingDINO/checkpoint/groundingdino_swint_ogc.pth\"\n",
        "\n",
        "# Check if the checkpoint file exists\n",
        "checkpoint_exists = os.path.exists(grounding_dino_checkpoint)\n",
        "print(f\"Grounding DINO Checkpoint Exists: {checkpoint_exists}\")\n",
        "\n",
        "# List the contents of the checkpoint directory to verify filenames\n",
        "if not checkpoint_exists:\n",
        "    print(f\"Contents of {grounding_dino_checkpoint_dir}:\")\n",
        "    print(os.listdir(grounding_dino_checkpoint_dir))\n"
      ],
      "metadata": {
        "id": "_IsbcItLYPK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 32: Monitor GPU Utilization\n",
        "\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "6BszpU_AzwDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 20: Verify SAM Checkpoint Path\n",
        "\n",
        "sam_checkpoint_path = \"/content/segment-anything/checkpoints/sam_vit_b_01ec64.pth\"\n",
        "\n",
        "# Check if the SAM checkpoint exists\n",
        "sam_checkpoint_exists = os.path.exists(sam_checkpoint_path)\n",
        "print(f\"SAM Checkpoint Exists: {sam_checkpoint_exists}\")\n",
        "\n",
        "# List the contents of the SAM checkpoint directory if missing\n",
        "if not sam_checkpoint_exists:\n",
        "    print(f\"Contents of /content/segment-anything/checkpoints/:\")\n",
        "    print(os.listdir(\"/content/segment-anything/checkpoints/\"))\n"
      ],
      "metadata": {
        "id": "3mNlVgwbYpA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 21: Load Grounding DINO and SAM Models\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Define paths (update these paths based on your Google Drive structure)\n",
        "grounding_dino_config = \"/content/drive/MyDrive/GroundingDINO/config/GroundingDINO_SwinT_OGC.py\"\n",
        "grounding_dino_checkpoint = \"/content/drive/MyDrive/GroundingDINO/checkpoint/groundingdino_swint_ogc.pth\"\n",
        "sam_checkpoint = \"/content/segment-anything/checkpoints/sam_vit_b_01ec64.pth\"  # Ensure SAM is downloaded here\n",
        "sam_model_type = \"vit_b\"  # Options: \"vit_b\", \"vit_l\", \"vit_h\"\n",
        "\n",
        "# Print the checkpoint paths to confirm\n",
        "print(f\"Grounding DINO Config Path: {grounding_dino_config}\")\n",
        "print(f\"Grounding DINO Checkpoint Path: {grounding_dino_checkpoint}\")\n",
        "print(f\"SAM Checkpoint Path: {sam_checkpoint}\")\n",
        "\n",
        "# Check if Grounding DINO checkpoint exists\n",
        "if not os.path.exists(grounding_dino_checkpoint):\n",
        "    print(\"❌ Grounding DINO checkpoint file not found. Please ensure it is downloaded correctly.\")\n",
        "else:\n",
        "    # Determine device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load Grounding DINO model\n",
        "    print(\"Loading Grounding DINO model...\")\n",
        "    grounding_dino_model = load_grounding_dino(\n",
        "        config_path=grounding_dino_config,\n",
        "        checkpoint_path=grounding_dino_checkpoint,\n",
        "        device=device\n",
        "    )\n",
        "    print(\"Grounding DINO model loaded successfully.\")\n",
        "\n",
        "# Check if SAM checkpoint exists\n",
        "if not os.path.exists(sam_checkpoint):\n",
        "    print(\"❌ SAM checkpoint file not found. Please ensure it is downloaded correctly.\")\n",
        "else:\n",
        "    # Load SAM predictor\n",
        "    print(\"Loading SAM model and initializing predictor...\")\n",
        "    sam_predictor = load_sam(\n",
        "        sam_checkpoint_path=sam_checkpoint,\n",
        "        model_type=sam_model_type,\n",
        "        device=device\n",
        "    )\n",
        "    print(\"SAM predictor initialized successfully.\")\n"
      ],
      "metadata": {
        "id": "8-qXLd0SNKR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 21b\n",
        "\n",
        "\n",
        "def get_grounding_output_updated(model, image, caption, box_threshold, text_threshold, device='cuda'):\n",
        "    \"\"\"\n",
        "    Perform Grounding DINO inference to obtain bounding boxes and associated phrases.\n",
        "\n",
        "    Args:\n",
        "        model: The loaded Grounding DINO model.\n",
        "        image (torch.Tensor): Preprocessed image tensor of shape (C, H, W).\n",
        "        caption (str): Text prompt containing multiple categories separated by commas.\n",
        "        box_threshold (float): Confidence threshold to filter bounding boxes.\n",
        "        text_threshold (float): Confidence threshold for text recognition.\n",
        "        device (str): Device to perform computation ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        boxes_filt (torch.Tensor): Filtered bounding boxes with shape (num_boxes, 4).\n",
        "        pred_phrases (list): List of predicted phrases corresponding to each bounding box.\n",
        "        scores (torch.Tensor): Confidence scores for each bounding box.\n",
        "    \"\"\"\n",
        "    image = image.to(device)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image[None], captions=[caption])\n",
        "\n",
        "    logits = outputs[\"pred_logits\"].sigmoid()[0]  # Shape: (num_queries, vocab_size)\n",
        "    boxes = outputs[\"pred_boxes\"][0]              # Shape: (num_queries, 4)\n",
        "\n",
        "    # Filter boxes with confidence threshold\n",
        "    logits_max, _ = logits.max(dim=1)\n",
        "    keep = logits_max > box_threshold\n",
        "    logits_filt = logits[keep]\n",
        "    boxes_filt = boxes[keep]\n",
        "\n",
        "    # Tokenize caption\n",
        "    tokenizer = model.tokenizer\n",
        "    tokenized = tokenizer(caption, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Map logits to phrases\n",
        "    pred_phrases = []\n",
        "    scores = []\n",
        "    for logit in logits_filt:\n",
        "        # Find tokens with confidence above text_threshold\n",
        "        token_indices = (logit > text_threshold).nonzero(as_tuple=True)[0]\n",
        "        if len(token_indices) == 0:\n",
        "            continue\n",
        "        tokens = tokenized[\"input_ids\"][0][token_indices]\n",
        "        phrase = tokenizer.decode(tokens)\n",
        "        pred_phrases.append(phrase)\n",
        "        scores.append(logit[token_indices].mean().item())\n",
        "\n",
        "    if len(scores) == 0:\n",
        "        return torch.empty((0, 4)).to(device), [], torch.empty((0,)).to(device)\n",
        "\n",
        "    boxes_filt = boxes_filt[:len(pred_phrases)]\n",
        "    scores = torch.tensor(scores).to(device)\n",
        "    return boxes_filt, pred_phrases, scores\n"
      ],
      "metadata": {
        "id": "_OBnYFhcMzfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJlgA_QrYvd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 22: Process Image with Grounding DINO and SAM\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "import traceback\n",
        "\n",
        "# Suppress FutureWarnings temporarily\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Define the image path\n",
        "image_path = \"/content/drive/MyDrive/multimodal_dataset/GT/outscene1208_10_0000000000.png\"\n",
        "\n",
        "# Verify if the image exists\n",
        "if not os.path.exists(image_path):\n",
        "    print(f\"❌ Image file not found at {image_path}. Please check the path.\")\n",
        "else:\n",
        "    # Load the image\n",
        "    image_pil, image_tensor = load_image(image_path)\n",
        "\n",
        "    # Move image tensor to the correct device\n",
        "    image_tensor = image_tensor.to(device)\n",
        "\n",
        "    # Display the image\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.imshow(image_pil)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.show()\n",
        "\n",
        "    # Updated prompts without 'a' and unnecessary words\n",
        "    prompts = [\n",
        "        \"concrete building\",\n",
        "        \"metal door\",\n",
        "        \"brick house\",\n",
        "        \"glass window\",\n",
        "        \"gravel pavement\",\n",
        "        \"wooden tree trunk\",\n",
        "        \"rubber motorcycle tire\",\n",
        "        \"cobblestone pathway\",\n",
        "        \"plastic bicycle frame\",\n",
        "        \"road marking lines\"\n",
        "    ]\n",
        "\n",
        "    # Combine prompts into a single caption\n",
        "    caption = ', '.join(prompts)\n",
        "\n",
        "    # Adjust thresholds as needed\n",
        "    box_threshold = 0.2\n",
        "    text_threshold = 0.1\n",
        "\n",
        "    print(f\"\\nProcessing caption: '{caption}'\")\n",
        "\n",
        "    try:\n",
        "        boxes_filt, pred_phrases, scores_filt = get_grounding_output_updated(\n",
        "            model=grounding_dino_model,\n",
        "            image=image_tensor,\n",
        "            caption=caption,\n",
        "            box_threshold=box_threshold,\n",
        "            text_threshold=text_threshold,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        print(f\"Detected {len(boxes_filt)} objects\")\n",
        "        for idx, (box, phrase) in enumerate(zip(boxes_filt, pred_phrases), 1):\n",
        "            print(f\"{idx}. {phrase} - Box coordinates: {box.cpu().numpy()}\")\n",
        "\n",
        "        # Use these boxes with SAM for segmentation\n",
        "        if boxes_filt.shape[0] > 0:\n",
        "            masks = segment_with_sam(\n",
        "                image_pil=image_pil,\n",
        "                boxes=boxes_filt,\n",
        "                predictor=sam_predictor\n",
        "            )\n",
        "\n",
        "            # Ensure output directory exists\n",
        "            output_dir = \"/content/drive/MyDrive/multimodal_dataset/output\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            # Visualization of the masks and save them\n",
        "            for idx, (mask, phrase) in enumerate(zip(masks, pred_phrases), 1):\n",
        "                plt.figure(figsize=(6,6))\n",
        "                plt.imshow(image_pil)\n",
        "                plt.imshow(mask, alpha=0.5, cmap='jet')\n",
        "                plt.title(f\"Mask {idx}: {phrase}\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Save the figure\n",
        "                mask_path = os.path.join(output_dir, f\"mask_{idx}.png\")\n",
        "                plt.savefig(mask_path)\n",
        "                print(f\"Saved Mask {idx} to {mask_path}\")\n",
        "\n",
        "                plt.show()\n",
        "        else:\n",
        "            print(\"No objects detected for segmentation.\")\n",
        "\n",
        "        # **Visualize Raw Scores**\n",
        "        if scores_filt.numel() > 0:\n",
        "            sns.set(style=\"whitegrid\")\n",
        "            plt.figure(figsize=(10,6))\n",
        "            sns.histplot(scores_filt.cpu().numpy().flatten(), bins=50, kde=True)\n",
        "            plt.title(\"Distribution of Filtered Scores\")\n",
        "            plt.xlabel(\"Score\")\n",
        "            plt.ylabel(\"Frequency\")\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"No raw scores available to visualize.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing caption: {e}\")\n",
        "        traceback.print_exc()\n"
      ],
      "metadata": {
        "id": "igU7Dmd8Y-G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aVWoRA6xUuWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lm72BPOUV9gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 23: Example Usage During Inference\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import traceback\n",
        "\n",
        "# Define the image path (update based on your Google Drive structure)\n",
        "image_path = \"/content/drive/MyDrive/multimodal_dataset/GT/outscene1208_10_0000000000.png\"  # Update as needed\n",
        "\n",
        "# Load the image\n",
        "image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Preprocess the image (ensure it matches the model's expected input)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                         std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "image_tensor = preprocess(image_pil).to(device)\n",
        "\n",
        "# Define your prompt (use multiple categories)\n",
        "prompt = \"sky, ground, building, tree, person\"  # Update based on your detection needs\n",
        "\n",
        "# Get grounding outputs\n",
        "try:\n",
        "    boxes_filt, pred_phrases, scores_filt = get_grounding_output_updated(\n",
        "        model=grounding_dino_model,  # Ensure 'grounding_dino_model' is correctly defined\n",
        "        image=image_tensor,\n",
        "        caption=prompt,  # Use 'caption' instead of 'prompt'\n",
        "        box_threshold=0.2,\n",
        "        text_threshold=0.1,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(f\"Detected {len(boxes_filt)} objects for prompt '{prompt}':\")\n",
        "    for idx, (box, phrase) in enumerate(zip(boxes_filt, pred_phrases), 1):\n",
        "        print(f\"{idx}. {phrase} - Box coordinates: {box.cpu().numpy()}\")\n",
        "\n",
        "    # Now you can use these boxes with SAM or for further processing\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing caption: {e}\")\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "id": "N180QpI2FzIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 24: Initialize TensorBoard (Optional)\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Define training output directory\n",
        "training_output_dir = \"/content/drive/MyDrive/MaterialSegmentationOutput\"\n",
        "model_checkpoint_dir = os.path.join(training_output_dir, \"checkpoints\")\n",
        "os.makedirs(model_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Initialize TensorBoard writer\n",
        "writer = SummaryWriter(log_dir=os.path.join(training_output_dir, \"tensorboard_logs\"))\n"
      ],
      "metadata": {
        "id": "1HJ9GKuk-azj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 25: Training and Validation Loop\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 25  # Adjust based on your requirements\n",
        "patience = 5  # For early stopping\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# Initialize GradScaler for mixed precision training\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, masks in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(images)['out']\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    print(f\"Train Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device).long()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(images)['out']\n",
        "                loss = criterion(outputs, masks)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == masks).sum().item()\n",
        "            total += masks.numel()\n",
        "\n",
        "    epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
        "    epoch_val_accuracy = correct / total\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "    print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {epoch_val_accuracy:.4f}\")\n",
        "\n",
        "    # Log metrics to TensorBoard\n",
        "    writer.add_scalar('Train/Loss', epoch_train_loss, epoch+1)\n",
        "    writer.add_scalar('Validation/Loss', epoch_val_loss, epoch+1)\n",
        "    writer.add_scalar('Validation/Accuracy', epoch_val_accuracy, epoch+1)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        counter = 0\n",
        "        # Save the best model\n",
        "        best_model_path = os.path.join(model_checkpoint_dir, \"best_model.pth\")\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved to {best_model_path}\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement in validation loss for {counter} epoch(s).\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # Save model checkpoint\n",
        "    checkpoint_path = os.path.join(model_checkpoint_dir, f\"model_epoch_{epoch + 1}.pth\")\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"Saved model checkpoint to {checkpoint_path}\\n\")\n"
      ],
      "metadata": {
        "id": "Z7kPrdaU-iQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 26: Save and Visualize Training Metrics\n",
        "\n",
        "# Save metrics to CSV\n",
        "metrics = {\n",
        "    \"Epoch\": list(range(1, len(train_losses) + 1)),\n",
        "    \"Train_Loss\": train_losses,\n",
        "    \"Validation_Loss\": val_losses,\n",
        "    \"Validation_Accuracy\": val_accuracies\n",
        "}\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "metrics_csv_path = os.path.join(training_output_dir, \"training_metrics.csv\")\n",
        "df_metrics.to_csv(metrics_csv_path, index=False)\n",
        "print(f\"Training metrics saved to {metrics_csv_path}\")\n",
        "\n",
        "# Plot Losses\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(df_metrics['Epoch'], df_metrics['Train_Loss'], label='Train Loss')\n",
        "plt.plot(df_metrics['Epoch'], df_metrics['Validation_Loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(df_metrics['Epoch'], df_metrics['Validation_Accuracy'], label='Validation Accuracy', color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d4z3Hr3e-oEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 27: Define CATEGORY_COLORS for Visualization\n",
        "\n",
        "# Define CATEGORY_COLORS\n",
        "CATEGORY_COLORS = {\n",
        "    \"background\": (0, 0, 0),\n",
        "    \"metal_object\": (255, 0, 0),\n",
        "    \"wooden_door\": (0, 255, 0),\n",
        "    \"concrete_wall\": (0, 0, 255),\n",
        "    \"glass_window\": (255, 255, 0),\n",
        "    \"soil\": (255, 165, 0),\n",
        "    \"bricks\": (128, 0, 128),\n",
        "    \"plastic\": (0, 255, 255),\n",
        "    \"tiles\": (255, 192, 203)\n",
        "}\n"
      ],
      "metadata": {
        "id": "4mTJF__b-ulu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 28: Visualize Segmentation Results\n",
        "\n",
        "def visualize_predictions(model, dataset, device, num_samples=5):\n",
        "    model.eval()\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "\n",
        "    for idx in indices:\n",
        "        image, mask = dataset[idx]\n",
        "        image_input = image.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image_input)['out'][0]\n",
        "            pred_mask = torch.argmax(output, dim=0).cpu().numpy()\n",
        "\n",
        "        # Convert tensors to NumPy arrays for visualization\n",
        "        image_np = image.cpu().numpy().transpose(1, 2, 0)\n",
        "        image_np = (image_np * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "        image_np = np.clip(image_np, 0, 1)\n",
        "\n",
        "        mask_np = mask.cpu().numpy()\n",
        "\n",
        "        # Create color masks\n",
        "        color_mask = np.zeros_like(image_np)\n",
        "        pred_color_mask = np.zeros_like(image_np)\n",
        "\n",
        "        for class_name, color in CATEGORY_COLORS.items():\n",
        "            class_num = list(CATEGORY_COLORS.keys()).index(class_name)  # Assuming background is 0\n",
        "            color_mask[mask_np == class_num] = np.array(color) / 255.0\n",
        "            pred_color_mask[pred_mask == class_num] = np.array(color) / 255.0\n",
        "\n",
        "        # Overlay masks on the image\n",
        "        overlay_true = (0.5 * image_np + 0.5 * color_mask)\n",
        "        overlay_pred = (0.5 * image_np + 0.5 * pred_color_mask)\n",
        "\n",
        "        # Plotting\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "        axs[0].imshow(image_np)\n",
        "        axs[0].set_title(\"Original Image\")\n",
        "        axs[0].axis('off')\n",
        "\n",
        "        axs[1].imshow(mask_np, cmap='jet', alpha=0.5)\n",
        "        axs[1].set_title(\"Ground Truth Mask\")\n",
        "        axs[1].axis('off')\n",
        "\n",
        "        axs[2].imshow(pred_mask, cmap='jet', alpha=0.5)\n",
        "        axs[2].set_title(\"Predicted Mask\")\n",
        "        axs[2].axis('off')\n",
        "\n",
        "        axs[3].imshow(overlay_pred)\n",
        "        axs[3].set_title(\"Overlay Predicted Mask\")\n",
        "        axs[3].axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# Visualize predictions on validation dataset\n",
        "visualize_predictions(model, val_dataset, device, num_samples=5)\n"
      ],
      "metadata": {
        "id": "bkmqpmq0-0NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 29: Example Usage During Inference\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the image path (update based on your Google Drive structure)\n",
        "image_path = \"/content/drive/MyDrive/multimodal_dataset/GT/outscene1208_10_0000000000.png\"  # Update as needed\n",
        "\n",
        "# Verify if the image exists\n",
        "if not os.path.exists(image_path):\n",
        "    print(f\"❌ Image file not found at {image_path}. Please check the path.\")\n",
        "else:\n",
        "    # Load the image\n",
        "    image_pil, image_tensor = load_image(image_path)\n",
        "\n",
        "    # Define your prompt\n",
        "    prompt = \"background\"  # Update based on your detection needs\n",
        "\n",
        "    # Get grounding outputs\n",
        "    boxes_filt, pred_phrases = get_grounding_output(\n",
        "        model=grounding_dino_model,  # Ensure 'grounding_dino_model' is correctly defined\n",
        "        image=image_tensor,\n",
        "        prompt=prompt,\n",
        "        box_threshold=0.2,\n",
        "        text_threshold=0.1,\n",
        "        cpu_only=False\n",
        "    )\n",
        "\n",
        "    print(f\"Detected {len(boxes_filt)} objects for prompt '{prompt}':\")\n",
        "    for idx, (box, phrase) in enumerate(zip(boxes_filt, pred_phrases), 1):\n",
        "        print(f\"{idx}. {phrase} - Box coordinates: {box.cpu().numpy()}\")\n",
        "\n",
        "    # Use these boxes with SAM for segmentation\n",
        "    if len(boxes_filt) > 0:\n",
        "        masks = segment_with_sam(\n",
        "            image_pil=image_pil,\n",
        "            boxes=boxes_filt,\n",
        "            predictor=sam_predictor\n",
        "        )\n",
        "\n",
        "        # Visualization of the masks\n",
        "        for idx, mask in enumerate(masks, 1):\n",
        "            plt.figure(figsize=(6,6))\n",
        "            plt.imshow(image_pil)\n",
        "            plt.imshow(mask, alpha=0.5, cmap='jet')\n",
        "            plt.title(f\"Mask {idx}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "    else:\n",
        "        print(\"No objects detected for segmentation.\")\n"
      ],
      "metadata": {
        "id": "EIsGXD0r-77a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 30: Evaluate Model on Test Set\n",
        "\n",
        "def evaluate_model(model, dataloader, device, num_classes):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "    iou_scores = np.zeros(num_classes)\n",
        "    intersection = np.zeros(num_classes)\n",
        "    union = np.zeros(num_classes)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device).long()\n",
        "\n",
        "            outputs = model(images)['out']\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # Calculate pixel-wise accuracy\n",
        "            total_correct += (preds == masks).sum().item()\n",
        "            total_pixels += masks.numel()\n",
        "\n",
        "            # Calculate IoU for each class\n",
        "            for cls in range(num_classes):\n",
        "                pred_inds = (preds == cls)\n",
        "                target_inds = (masks == cls)\n",
        "                intersection[cls] += (pred_inds & target_inds).sum().item()\n",
        "                union[cls] += (pred_inds | target_inds).sum().item()\n",
        "\n",
        "    accuracy = total_correct / total_pixels\n",
        "    iou = intersection / union\n",
        "    mean_iou = np.nanmean(iou)\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    for cls in range(num_classes):\n",
        "        print(f\"Class {cls} IoU: {iou[cls]:.4f}\")\n",
        "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
        "\n",
        "# Define number of classes\n",
        "num_classes = 9  # Adjust based on your dataset\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate_model(model, test_loader, device, num_classes)\n"
      ],
      "metadata": {
        "id": "-XDGoJMq_Igu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 31: Visualize Model Predictions on Test Set\n",
        "\n",
        "def visualize_test_predictions(model, dataset, device, num_samples=5):\n",
        "    model.eval()\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "\n",
        "    for idx in indices:\n",
        "        image, mask = dataset[idx]\n",
        "        image_input = image.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image_input)['out'][0]\n",
        "            pred_mask = torch.argmax(output, dim=0).cpu().numpy()\n",
        "\n",
        "        # Convert tensors to NumPy arrays for visualization\n",
        "        image_np = image.cpu().numpy().transpose(1, 2, 0)\n",
        "        image_np = (image_np * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "        image_np = np.clip(image_np, 0, 1)\n",
        "\n",
        "        mask_np = mask.cpu().numpy()\n",
        "\n",
        "        # Create color masks\n",
        "        color_mask = np.zeros_like(image_np)\n",
        "        pred_color_mask = np.zeros_like(image_np)\n",
        "\n",
        "        for class_name, color in CATEGORY_COLORS.items():\n",
        "            class_num = list(CATEGORY_COLORS.keys()).index(class_name)  # Assuming background is 0\n",
        "            color_mask[mask_np == class_num] = np.array(color) / 255.0\n",
        "            pred_color_mask[pred_mask == class_num] = np.array(color) / 255.0\n",
        "\n",
        "        # Overlay masks on the image\n",
        "        overlay_true = (0.5 * image_np + 0.5 * color_mask)\n",
        "        overlay_pred = (0.5 * image_np + 0.5 * pred_color_mask)\n",
        "\n",
        "        # Plotting\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "        axs[0].imshow(image_np)\n",
        "        axs[0].set_title(\"Original Image\")\n",
        "        axs[0].axis('off')\n",
        "\n",
        "        axs[1].imshow(mask_np, cmap='jet', alpha=0.5)\n",
        "        axs[1].set_title(\"Ground Truth Mask\")\n",
        "        axs[1].axis('off')\n",
        "\n",
        "        axs[2].imshow(pred_mask, cmap='jet', alpha=0.5)\n",
        "        axs[2].set_title(\"Predicted Mask\")\n",
        "        axs[2].axis('off')\n",
        "\n",
        "        axs[3].imshow(overlay_pred)\n",
        "        axs[3].set_title(\"Overlay Predicted Mask\")\n",
        "        axs[3].axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# Visualize predictions on test dataset\n",
        "visualize_test_predictions(model, test_dataset, device, num_samples=5)\n"
      ],
      "metadata": {
        "id": "PVJEvsr1_MLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 32: Monitor GPU Utilization\n",
        "\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "Ru1JVMg5_O5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 33: Save Confidence Scores (Optional)\n",
        "\n",
        "# Initialize a list to store confidence data before the training loop\n",
        "confidence_records = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, masks in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(images)['out']\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    print(f\"Train Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device).long()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(images)['out']\n",
        "                loss = criterion(outputs, masks)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == masks).sum().item()\n",
        "            total += masks.numel()\n",
        "\n",
        "    epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
        "    epoch_val_accuracy = correct / total\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "    print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {epoch_val_accuracy:.4f}\")\n",
        "\n",
        "    # Record confidence scores (example: maximum predicted probability per sample)\n",
        "    confidence_scores = torch.max(torch.softmax(outputs, dim=1), dim=1)[0].cpu().numpy()\n",
        "    for idx, conf in enumerate(confidence_scores):\n",
        "        confidence_records.append({\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Sample\": idx,\n",
        "            \"Confidence\": float(conf)\n",
        "        })\n",
        "\n",
        "    # Log metrics to TensorBoard\n",
        "    writer.add_scalar('Train/Loss', epoch_train_loss, epoch+1)\n",
        "    writer.add_scalar('Validation/Loss', epoch_val_loss, epoch+1)\n",
        "    writer.add_scalar('Validation/Accuracy', epoch_val_accuracy, epoch+1)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        counter = 0\n",
        "        # Save the best model\n",
        "        best_model_path = os.path.join(model_checkpoint_dir, \"best_model.pth\")\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved to {best_model_path}\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement in validation loss for {counter} epoch(s).\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # Save model checkpoint\n",
        "    checkpoint_path = os.path.join(model_checkpoint_dir, f\"model_epoch_{epoch + 1}.pth\")\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"Saved model checkpoint to {checkpoint_path}\\n\")\n",
        "\n",
        "# After Training, Save Confidence Records to CSV\n",
        "df_confidence = pd.DataFrame(confidence_records)\n",
        "confidence_csv_path = os.path.join(training_output_dir, \"confidence_scores.csv\")\n",
        "df_confidence.to_csv(confidence_csv_path, index=False)\n",
        "print(f\"Confidence scores saved to {confidence_csv_path}\")\n"
      ],
      "metadata": {
        "id": "fQ0SqcPV_Vxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 34: Visualize Confidence Scores (Optional)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load confidence scores\n",
        "confidence_csv_path = os.path.join(training_output_dir, \"confidence_scores.csv\")\n",
        "df_confidence = pd.read_csv(confidence_csv_path)\n",
        "\n",
        "# Plot Confidence Scores Over Epochs\n",
        "plt.figure(figsize=(10,5))\n",
        "for sample in df_confidence['Sample'].unique():\n",
        "    sample_data = df_confidence[df_confidence['Sample'] == sample]\n",
        "    plt.plot(sample_data['Epoch'], sample_data['Confidence'], label=f'Sample {sample}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Confidence')\n",
        "plt.title('Confidence Scores Over Epochs')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XtqYlITI_gtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 35: Setup TensorBoard\n",
        "\n",
        "# Launch TensorBoard\n",
        "%tensorboard --logdir /content/drive/MyDrive/MaterialSegmentationOutput/tensorboard_logs\n"
      ],
      "metadata": {
        "id": "OWbJjhAV_kB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 36: Integrate SAM with Fine-Tuned Grounding DINO\n",
        "\n",
        "# Define SAM checkpoint and model type\n",
        "sam_checkpoint = \"/content/segment-anything/checkpoints/sam_vit_b_01ec64.pth\"  # Update path if different\n",
        "sam_model_type = \"vit_b\"  # Options: \"vit_b\", \"vit_l\", \"vit_h\"\n",
        "\n",
        "# Load SAM model\n",
        "sam = sam_model_registry[sam_model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device)\n",
        "sam_predictor = SamPredictor(sam)\n",
        "\n",
        "def segment_with_sam(image_pil, boxes, predictor):\n",
        "    \"\"\"\n",
        "    Perform segmentation using the SAM model based on bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        image_pil (PIL.Image.Image): The input image.\n",
        "        boxes (torch.Tensor): Bounding boxes detected by Grounding DINO.\n",
        "        predictor: The SAM predictor object.\n",
        "\n",
        "    Returns:\n",
        "        List of NumPy arrays representing segmentation masks.\n",
        "    \"\"\"\n",
        "    image_np = np.array(image_pil)\n",
        "    predictor.set_image(image_np)\n",
        "    masks = []\n",
        "\n",
        "    for box in boxes:\n",
        "        # Convert box to XYWH format\n",
        "        x_min, y_min, x_max, y_max = box.cpu().numpy()\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "        sam_box = np.array([x_min, y_min, width, height])\n",
        "\n",
        "        # Perform prediction\n",
        "        masks_pred, _, _ = predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=sam_box,\n",
        "            multimask_output=False,\n",
        "        )\n",
        "\n",
        "        # Convert mask to NumPy array\n",
        "        mask_np = masks_pred[0].cpu().numpy()\n",
        "        masks.append(mask_np)\n",
        "\n",
        "    return masks\n"
      ],
      "metadata": {
        "id": "yh4ySAzF_oEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 37: Evaluate Model with IoU and Accuracy Metrics\n",
        "\n",
        "def calculate_iou(pred, target, num_classes):\n",
        "    ious = []\n",
        "    pred = pred.flatten()\n",
        "    target = target.flatten()\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = pred == cls\n",
        "        target_inds = target == cls\n",
        "        intersection = (pred_inds & target_inds).sum()\n",
        "        union = (pred_inds | target_inds).sum()\n",
        "        if union == 0:\n",
        "            ious.append(float('nan'))  # If no ground truth, do not include in evaluation\n",
        "        else:\n",
        "            ious.append(intersection / union)\n",
        "    return ious\n",
        "\n",
        "def evaluate(model, dataloader, device, num_classes):\n",
        "    model.eval()\n",
        "    iou_scores = []\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device).long()\n",
        "\n",
        "            outputs = model(images)['out']\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            for pred, mask in zip(preds, masks):\n",
        "                iou = calculate_iou(pred.cpu().numpy(), mask.cpu().numpy(), num_classes)\n",
        "                iou_scores.append(iou)\n",
        "    # Calculate mean IoU for each class\n",
        "    iou_scores = np.array(iou_scores)\n",
        "    mean_ious = np.nanmean(iou_scores, axis=0)\n",
        "    for cls_idx, mean_iou in enumerate(mean_ious):\n",
        "        print(f\"Class {cls_idx}: Mean IoU = {mean_iou:.4f}\")\n",
        "    # Overall Mean IoU\n",
        "    overall_mean_iou = np.nanmean(mean_ious)\n",
        "    print(f\"Overall Mean IoU: {overall_mean_iou:.4f}\")\n",
        "\n",
        "# Define number of classes\n",
        "num_classes = 9  # Adjust based on your dataset\n",
        "\n",
        "# Evaluate on validation set\n",
        "evaluate(model, val_loader, device, num_classes)\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate(model, test_loader, device, num_classes)\n"
      ],
      "metadata": {
        "id": "Y08PUY36_435"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 38: Implement Early Stopping and Save Best Model\n",
        "\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, masks in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(images)['out']\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    print(f\"Train Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device).long()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(images)['out']\n",
        "                loss = criterion(outputs, masks)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == masks).sum().item()\n",
        "            total += masks.numel()\n",
        "\n",
        "    epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
        "    epoch_val_accuracy = correct / total\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "    print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {epoch_val_accuracy:.4f}\")\n",
        "\n",
        "    # Log metrics to TensorBoard\n",
        "    writer.add_scalar('Train/Loss', epoch_train_loss, epoch+1)\n",
        "    writer.add_scalar('Validation/Loss', epoch_val_loss, epoch+1)\n",
        "    writer.add_scalar('Validation/Accuracy', epoch_val_accuracy, epoch+1)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        counter = 0\n",
        "        # Save the best model\n",
        "        best_model_path = os.path.join(model_checkpoint_dir, \"best_model.pth\")\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved to {best_model_path}\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement in validation loss for {counter} epoch(s).\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # Save model checkpoint\n",
        "    checkpoint_path = os.path.join(model_checkpoint_dir, f\"model_epoch_{epoch + 1}.pth\")\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"Saved model checkpoint to {checkpoint_path}\\n\")\n"
      ],
      "metadata": {
        "id": "bVAEKQXV_-DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 39: Integrate TensorBoard Logs\n",
        "\n",
        "# Launch TensorBoard\n",
        "%tensorboard --logdir /content/drive/MyDrive/MaterialSegmentationOutput/tensorboard_logs\n"
      ],
      "metadata": {
        "id": "gKQ0HJNgAds_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 40: Final Evaluation on Test Set\n",
        "\n",
        "# Load the best model\n",
        "best_model_path = os.path.join(model_checkpoint_dir, \"best_model.pth\")\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(f\"Loaded best model from {best_model_path}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate(model, test_loader, device, num_classes)\n",
        "\n",
        "# Visualize some test predictions\n",
        "visualize_test_predictions(model, test_dataset, device, num_samples=5)\n"
      ],
      "metadata": {
        "id": "NppyFXCkAicn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 41: Save the Trained Model\n",
        "\n",
        "final_model_path = os.path.join(model_checkpoint_dir, \"final_model.pth\")\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"Final model saved to {final_model_path}\")\n"
      ],
      "metadata": {
        "id": "7oX7fDpSAqJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 42: Shutdown TensorBoard (Optional)\n",
        "\n",
        "# To stop TensorBoard, interrupt the cell execution or run the following:\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "JR5jGFPnAuHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jqNp7mIh_UJh"
      }
    }
  ]
}