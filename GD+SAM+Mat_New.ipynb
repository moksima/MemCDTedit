{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNnWqDTdYhT4kgGh0WyDYF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moksima/MemCDTedit/blob/main/GD%2BSAM%2BMat_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VDNE6GFkV8Rz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Clone the Repository Safely\n",
        "\n",
        "import os\n",
        "\n",
        "repo_url = \"https://github.com/kyotovision-public/multimodal-material-segmentation.git\"\n",
        "repo_dir = \"multimodal-material-segmentation\"\n",
        "\n",
        "if not os.path.exists(repo_dir):\n",
        "    !git clone {repo_url}\n",
        "    print(f\"Cloned repository '{repo_dir}'.\")\n",
        "else:\n",
        "    print(f\"Repository '{repo_dir}' already exists. Pulling latest changes.\")\n",
        "    !cd {repo_dir} && git pull\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CkWAb2TmWr_",
        "outputId": "215a6334-db67-41dd-ebe4-3423727c6136"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'multimodal-material-segmentation'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
            "remote: Total 164 (delta 42), reused 148 (delta 33), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (164/164), 2.75 MiB | 24.89 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n",
            "Cloned repository 'multimodal-material-segmentation'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Install Dependencies with Compatible Versions\n",
        "\n",
        "%cd multimodal-material-segmentation\n",
        "\n",
        "# Upgrade pip, setuptools, and wheel\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "\n",
        "# Install dependencies without strict version pinning to ensure compatibility with Python 3.10\n",
        "# You can customize this list based on your project's actual dependencies\n",
        "!pip install absl-py aiohttp astroid async-timeout attrs cachetools certifi chardet cycler Cython future \\\n",
        "    google-auth google-auth-oauthlib grpcio idna idna-ssl importlib-metadata isort kiwisolver \\\n",
        "    lazy-object-proxy Markdown matplotlib mccabe multidict numpy transformers filelock huggingface-hub \\\n",
        "    safetensors tokenizers tqdm opencv-python addict pycocotools supervision timm yapf\n",
        "\n",
        "# Additionally, install other necessary packages\n",
        "!pip install torch torchvision albumentations numpy matplotlib scikit-learn opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "374ZjarHWFbS",
        "outputId": "c92a3470-fcc5-4d25-eaeb-7b12c74086a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/multimodal-material-segmentation/multimodal-material-segmentation\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (75.3.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.44.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.10)\n",
            "Requirement already satisfied: astroid in /usr/local/lib/python3.10/dist-packages (3.3.5)\n",
            "Requirement already satisfied: async-timeout in /usr/local/lib/python3.10/dist-packages (4.0.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (24.2.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (2024.8.30)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (3.0.11)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (1.64.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (3.10)\n",
            "Requirement already satisfied: idna-ssl in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (8.5.0)\n",
            "Requirement already satisfied: isort in /usr/local/lib/python3.10/dist-packages (5.13.2)\n",
            "Requirement already satisfied: kiwisolver in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: lazy-object-proxy in /usr/local/lib/python3.10/dist-packages (1.10.0)\n",
            "Requirement already satisfied: Markdown in /usr/local/lib/python3.10/dist-packages (3.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: mccabe in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: multidict in /usr/local/lib/python3.10/dist-packages (6.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.5)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.11)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (0.40.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from astroid) (4.12.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata) (3.20.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.10.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.13.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.0+cu121)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf) (4.3.6)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf) (2.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (0.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.21)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.20 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.20)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.20->albumentations) (3.10.6)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.20->albumentations) (6.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3a: Install Grounding DINO and Handle Existing Clones\n",
        "\n",
        "import os\n",
        "\n",
        "grounding_dino_repo = \"GroundingDINO\"\n",
        "grounding_dino_url = \"https://github.com/IDEA-Research/GroundingDINO.git\"\n",
        "\n",
        "if not os.path.exists(grounding_dino_repo):\n",
        "    !git clone {grounding_dino_url}\n",
        "    print(f\"Cloned GroundingDINO repository.\")\n",
        "else:\n",
        "    print(f\"GroundingDINO repository already exists. Pulling latest changes.\")\n",
        "    !cd {grounding_dino_repo} && git pull\n",
        "\n",
        "%cd GroundingDINO\n",
        "\n",
        "# Upgrade pip and install requirements\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "\n",
        "# Install GroundingDINO in editable mode\n",
        "!pip install -e .\n",
        "\n",
        "%cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trLhUDZuWMaz",
        "outputId": "0b26e59d-cebb-4ca0-94f2-0b5c01708b71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GroundingDINO'...\n",
            "remote: Enumerating objects: 463, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/240)\u001b[K\rremote: Counting objects:   1% (3/240)\u001b[K\rremote: Counting objects:   2% (5/240)\u001b[K\rremote: Counting objects:   3% (8/240)\u001b[K\rremote: Counting objects:   4% (10/240)\u001b[K\rremote: Counting objects:   5% (12/240)\u001b[K\rremote: Counting objects:   6% (15/240)\u001b[K\rremote: Counting objects:   7% (17/240)\u001b[K\rremote: Counting objects:   8% (20/240)\u001b[K\rremote: Counting objects:   9% (22/240)\u001b[K\rremote: Counting objects:  10% (24/240)\u001b[K\rremote: Counting objects:  11% (27/240)\u001b[K\rremote: Counting objects:  12% (29/240)\u001b[K\rremote: Counting objects:  13% (32/240)\u001b[K\rremote: Counting objects:  14% (34/240)\u001b[K\rremote: Counting objects:  15% (36/240)\u001b[K\rremote: Counting objects:  16% (39/240)\u001b[K\rremote: Counting objects:  17% (41/240)\u001b[K\rremote: Counting objects:  18% (44/240)\u001b[K\rremote: Counting objects:  19% (46/240)\u001b[K\rremote: Counting objects:  20% (48/240)\u001b[K\rremote: Counting objects:  21% (51/240)\u001b[K\rremote: Counting objects:  22% (53/240)\u001b[K\rremote: Counting objects:  23% (56/240)\u001b[K\rremote: Counting objects:  24% (58/240)\u001b[K\rremote: Counting objects:  25% (60/240)\u001b[K\rremote: Counting objects:  26% (63/240)\u001b[K\rremote: Counting objects:  27% (65/240)\u001b[K\rremote: Counting objects:  28% (68/240)\u001b[K\rremote: Counting objects:  29% (70/240)\u001b[K\rremote: Counting objects:  30% (72/240)\u001b[K\rremote: Counting objects:  31% (75/240)\u001b[K\rremote: Counting objects:  32% (77/240)\u001b[K\rremote: Counting objects:  33% (80/240)\u001b[K\rremote: Counting objects:  34% (82/240)\u001b[K\rremote: Counting objects:  35% (84/240)\u001b[K\rremote: Counting objects:  36% (87/240)\u001b[K\rremote: Counting objects:  37% (89/240)\u001b[K\rremote: Counting objects:  38% (92/240)\u001b[K\rremote: Counting objects:  39% (94/240)\u001b[K\rremote: Counting objects:  40% (96/240)\u001b[K\rremote: Counting objects:  41% (99/240)\u001b[K\rremote: Counting objects:  42% (101/240)\u001b[K\rremote: Counting objects:  43% (104/240)\u001b[K\rremote: Counting objects:  44% (106/240)\u001b[K\rremote: Counting objects:  45% (108/240)\u001b[K\rremote: Counting objects:  46% (111/240)\u001b[K\rremote: Counting objects:  47% (113/240)\u001b[K\rremote: Counting objects:  48% (116/240)\u001b[K\rremote: Counting objects:  49% (118/240)\u001b[K\rremote: Counting objects:  50% (120/240)\u001b[K\rremote: Counting objects:  51% (123/240)\u001b[K\rremote: Counting objects:  52% (125/240)\u001b[K\rremote: Counting objects:  53% (128/240)\u001b[K\rremote: Counting objects:  54% (130/240)\u001b[K\rremote: Counting objects:  55% (132/240)\u001b[K\rremote: Counting objects:  56% (135/240)\u001b[K\rremote: Counting objects:  57% (137/240)\u001b[K\rremote: Counting objects:  58% (140/240)\u001b[K\rremote: Counting objects:  59% (142/240)\u001b[K\rremote: Counting objects:  60% (144/240)\u001b[K\rremote: Counting objects:  61% (147/240)\u001b[K\rremote: Counting objects:  62% (149/240)\u001b[K\rremote: Counting objects:  63% (152/240)\u001b[K\rremote: Counting objects:  64% (154/240)\u001b[K\rremote: Counting objects:  65% (156/240)\u001b[K\rremote: Counting objects:  66% (159/240)\u001b[K\rremote: Counting objects:  67% (161/240)\u001b[K\rremote: Counting objects:  68% (164/240)\u001b[K\rremote: Counting objects:  69% (166/240)\u001b[K\rremote: Counting objects:  70% (168/240)\u001b[K\rremote: Counting objects:  71% (171/240)\u001b[K\rremote: Counting objects:  72% (173/240)\u001b[K\rremote: Counting objects:  73% (176/240)\u001b[K\rremote: Counting objects:  74% (178/240)\u001b[K\rremote: Counting objects:  75% (180/240)\u001b[K\rremote: Counting objects:  76% (183/240)\u001b[K\rremote: Counting objects:  77% (185/240)\u001b[K\rremote: Counting objects:  78% (188/240)\u001b[K\rremote: Counting objects:  79% (190/240)\u001b[K\rremote: Counting objects:  80% (192/240)\u001b[K\rremote: Counting objects:  81% (195/240)\u001b[K\rremote: Counting objects:  82% (197/240)\u001b[K\rremote: Counting objects:  83% (200/240)\u001b[K\rremote: Counting objects:  84% (202/240)\u001b[K\rremote: Counting objects:  85% (204/240)\u001b[K\rremote: Counting objects:  86% (207/240)\u001b[K\rremote: Counting objects:  87% (209/240)\u001b[K\rremote: Counting objects:  88% (212/240)\u001b[K\rremote: Counting objects:  89% (214/240)\u001b[K\rremote: Counting objects:  90% (216/240)\u001b[K\rremote: Counting objects:  91% (219/240)\u001b[K\rremote: Counting objects:  92% (221/240)\u001b[K\rremote: Counting objects:  93% (224/240)\u001b[K\rremote: Counting objects:  94% (226/240)\u001b[K\rremote: Counting objects:  95% (228/240)\u001b[K\rremote: Counting objects:  96% (231/240)\u001b[K\rremote: Counting objects:  97% (233/240)\u001b[K\rremote: Counting objects:  98% (236/240)\u001b[K\rremote: Counting objects:  99% (238/240)\u001b[K\rremote: Counting objects: 100% (240/240)\u001b[K\rremote: Counting objects: 100% (240/240), done.\u001b[K\n",
            "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 463 (delta 175), reused 136 (delta 136), pack-reused 223 (from 1)\u001b[K\n",
            "Receiving objects: 100% (463/463), 12.87 MiB | 21.71 MiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n",
            "Cloned GroundingDINO repository.\n",
            "/content/multimodal-material-segmentation/multimodal-material-segmentation/GroundingDINO\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (75.3.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.44.0)\n",
            "Obtaining file:///content/multimodal-material-segmentation/multimodal-material-segmentation/GroundingDINO\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (0.20.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (4.44.2)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (0.40.2)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (1.0.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (4.10.0.84)\n",
            "Requirement already satisfied: supervision>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (0.24.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (2.0.8)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (3.8.0)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.10/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->groundingdino==0.1.0) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->groundingdino==0.1.0) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->groundingdino==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (4.66.6)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino==0.1.0) (8.5.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino==0.1.0) (4.3.6)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->groundingdino==0.1.0) (3.20.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->groundingdino==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->groundingdino==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->groundingdino==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->groundingdino==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->groundingdino==0.1.0) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (1.16.0)\n",
            "Installing collected packages: groundingdino\n",
            "  Attempting uninstall: groundingdino\n",
            "    Found existing installation: groundingdino 0.1.0\n",
            "    Uninstalling groundingdino-0.1.0:\n",
            "      Successfully uninstalled groundingdino-0.1.0\n",
            "\u001b[33m  DEPRECATION: Legacy editable install of groundingdino==0.1.0 from file:///content/multimodal-material-segmentation/multimodal-material-segmentation/GroundingDINO (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running setup.py develop for groundingdino\n",
            "Successfully installed groundingdino\n",
            "/content/multimodal-material-segmentation/multimodal-material-segmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3b: Install 'segment-anything' and Upgrade 'albumentations'\n",
        "\n",
        "# Install segment-anything via pip\n",
        "!pip install segment-anything\n",
        "\n",
        "# Upgrade albumentations to the latest version to resolve the warning\n",
        "!pip install --upgrade albumentations\n",
        "\n",
        "print(\"Installed 'segment-anything' and upgraded 'albumentations' successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrCtayGlkLZp",
        "outputId": "71065e26-0fa8-4938-93b4-1168bd5360c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segment-anything in /usr/local/lib/python3.10/dist-packages (1.0)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.21)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.20 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.20)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.20->albumentations) (3.10.6)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.20->albumentations) (6.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Installed 'segment-anything' and upgraded 'albumentations' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Install Hugging Face Hub and Authenticate\n",
        "\n",
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# Securely input your Hugging Face Token\n",
        "hf_token = getpass(\"Enter your Hugging Face API Token: \")\n",
        "login(token=hf_token)\n",
        "\n",
        "# Optionally, set the token as an environment variable\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "print(\"Hugging Face login successful.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is_08JFjWOaG",
        "outputId": "0cc5edbe-c2fa-403d-806f-98145167a169"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n",
            "Enter your Hugging Face API Token: ··········\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Hugging Face login successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Import Necessary Libraries\n",
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "from google.colab import drive\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from groundingdino.models import build_model\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.utils import clean_state_dict\n",
        "from transformers import BertTokenizer\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings (optional)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"All libraries imported successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "D69T38FlWdLq",
        "outputId": "d040825e-134d-4ad9-d70b-165468f3b41f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'groundingdino'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-61d0e0856bbd>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSLConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclean_state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'groundingdino'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "print(\"Google Drive mounted successfully.\")\n"
      ],
      "metadata": {
        "id": "iy5qtF6MWe6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Define Custom Dataset for MCubeS\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class MaterialSegmentationDataset(Dataset):\n",
        "    def __init__(self, list_file, images_dirs, masks_dirs, transforms=None,\n",
        "                 image_extensions=['.jpg', '.jpeg', '.png'],\n",
        "                 mask_extensions=['.npy', '.png', '.jpg', '.jpeg'],\n",
        "                 grayscale_to_class=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            list_file (str): Path to the txt file containing filenames for the split.\n",
        "            images_dirs (list): List of directories containing images.\n",
        "            masks_dirs (list): List of directories containing masks.\n",
        "            transforms (albumentations.Compose, optional): Transformations to apply.\n",
        "            image_extensions (list): List of acceptable image file extensions.\n",
        "            mask_extensions (list): List of acceptable mask file extensions.\n",
        "            grayscale_to_class (dict, optional): Mapping from grayscale values to class indices.\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "        self.images_dirs = images_dirs\n",
        "        self.masks_dirs = masks_dirs\n",
        "        self.image_extensions = image_extensions\n",
        "        self.mask_extensions = mask_extensions\n",
        "        self.grayscale_to_class = grayscale_to_class if grayscale_to_class else {}\n",
        "\n",
        "        # Read the list of filenames\n",
        "        with open(list_file, 'r') as f:\n",
        "            self.filenames = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        # Create a mapping from filename to image path\n",
        "        self.image_paths = []\n",
        "        missing_images = []\n",
        "        for fname in self.filenames:\n",
        "            found = False\n",
        "            for img_dir in self.images_dirs:\n",
        "                for ext in self.image_extensions:\n",
        "                    potential_path = os.path.join(img_dir, fname + ext)\n",
        "                    if os.path.isfile(potential_path):\n",
        "                        self.image_paths.append(potential_path)\n",
        "                        found = True\n",
        "                        break  # Stop searching extensions after finding the image\n",
        "                if found:\n",
        "                    break  # Stop searching directories after finding the image\n",
        "            if not found:\n",
        "                missing_images.append(fname)\n",
        "\n",
        "        if missing_images:\n",
        "            print(f\"Total missing images: {len(missing_images)}\")\n",
        "            for msg in missing_images[:5]:  # Print first 5 missing images\n",
        "                print(f\"Missing image: {msg}\")\n",
        "\n",
        "        # Create a mapping from filename to mask path\n",
        "        self.mask_paths = []\n",
        "        missing_masks = []\n",
        "        for fname in self.filenames:\n",
        "            found = False\n",
        "            for mask_dir in self.masks_dirs:\n",
        "                for ext in self.mask_extensions:\n",
        "                    potential_mask_path = os.path.join(mask_dir, fname + ext)\n",
        "                    if os.path.isfile(potential_mask_path):\n",
        "                        self.mask_paths.append(potential_mask_path)\n",
        "                        found = True\n",
        "                        break  # Stop searching extensions after finding the mask\n",
        "            if not found:\n",
        "                missing_masks.append(fname)\n",
        "\n",
        "        if missing_masks:\n",
        "            print(f\"Total missing masks: {len(missing_masks)}\")\n",
        "            for msg in missing_masks[:5]:  # Print first 5 missing masks\n",
        "                print(f\"Missing mask: {msg}\")\n",
        "\n",
        "        # Ensure that each image has a corresponding mask\n",
        "        assert len(self.image_paths) == len(self.mask_paths), \"Mismatch between images and masks.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "\n",
        "        # Load mask\n",
        "        mask_path = self.mask_paths[idx]\n",
        "        if mask_path.endswith('.npy'):\n",
        "            mask = np.load(mask_path)\n",
        "            # Ensure mask is in the correct format (e.g., single channel)\n",
        "            if mask.ndim == 3:\n",
        "                mask = mask[:, :, 0]\n",
        "        elif mask_path.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
        "            mask = np.array(mask)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported mask format: {mask_path}\")\n",
        "\n",
        "        # Map mask values to class indices\n",
        "        mask = self.map_mask(mask, self.grayscale_to_class)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transforms:\n",
        "            augmented = self.transforms(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        # Ensure mask is of type torch.LongTensor\n",
        "        if isinstance(mask, np.ndarray):\n",
        "            mask = torch.from_numpy(mask).long()\n",
        "        elif isinstance(mask, torch.Tensor):\n",
        "            mask = mask.long()\n",
        "        else:\n",
        "            raise TypeError(f\"Unsupported mask type: {type(mask)}\")\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def map_mask(self, mask, mapping):\n",
        "        mapped_mask = np.full_like(mask, fill_value=-1, dtype=np.int64)  # Initialize with -1\n",
        "        for grayscale_value, class_index in mapping.items():\n",
        "            mapped_mask[mask == grayscale_value] = class_index\n",
        "\n",
        "        # Check for unmapped values\n",
        "        unmapped_pixels = (mapped_mask == -1)\n",
        "        if np.any(unmapped_pixels):\n",
        "            unique_unmapped_values = np.unique(mask[unmapped_pixels])\n",
        "            print(f\"Warning: Found unmapped grayscale values in mask: {unique_unmapped_values}\")\n",
        "            # Assign unmapped pixels to background class or any other class\n",
        "            mapped_mask[unmapped_pixels] = 0  # Assign to background class\n",
        "\n",
        "        return mapped_mask\n"
      ],
      "metadata": {
        "id": "w7eLvMlIWhxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Collect All Unique Grayscale Values from Train, Validation, and Test Sets\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define directories containing images and masks\n",
        "images_dirs = [\n",
        "    \"/content/drive/MyDrive/multimodal_dataset/polL_color\"\n",
        "    # Add more image directories if applicable\n",
        "]\n",
        "\n",
        "masks_dirs = [\n",
        "    \"/content/drive/MyDrive/multimodal_dataset/GT\"\n",
        "    # Add more mask directories if applicable\n",
        "]\n",
        "\n",
        "# Define list files\n",
        "train_list = \"/content/drive/MyDrive/multimodal_dataset/list_folder/train.txt\"\n",
        "val_list = \"/content/drive/MyDrive/multimodal_dataset/list_folder/val.txt\"\n",
        "test_list = \"/content/drive/MyDrive/multimodal_dataset/list_folder/test.txt\"\n",
        "\n",
        "# Initialize datasets for all splits to collect unique grayscale values\n",
        "train_dataset = MaterialSegmentationDataset(\n",
        "    list_file=train_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=None  # No transforms needed\n",
        ")\n",
        "\n",
        "val_dataset = MaterialSegmentationDataset(\n",
        "    list_file=val_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=None  # No transforms needed\n",
        ")\n",
        "\n",
        "test_dataset = MaterialSegmentationDataset(\n",
        "    list_file=test_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=None  # No transforms needed\n",
        ")\n",
        "\n",
        "# Collect unique grayscale values from all splits\n",
        "unique_grayscale = set()\n",
        "\n",
        "print(\"Collecting unique grayscale values from all splits...\")\n",
        "for split_name, dataset in zip(['Train', 'Validation', 'Test'], [train_dataset, val_dataset, test_dataset]):\n",
        "    split_unique = set()\n",
        "    for _, mask in tqdm(dataset, desc=f\"Processing {split_name} Set\", leave=False):\n",
        "        split_unique.update(np.unique(mask.numpy()))\n",
        "    unique_grayscale.update(split_unique)\n",
        "    print(f\"Unique grayscale values in {split_name} set: {sorted(split_unique)}\")\n",
        "\n",
        "print(f\"\\nTotal unique grayscale values across all splits: {sorted(unique_grayscale)}\")\n",
        "\n",
        "# Define the comprehensive grayscale_to_class mapping\n",
        "grayscale_to_class = {\n",
        "    0: 0,     # Background\n",
        "    1: 1,     # Asphalt\n",
        "    2: 2,     # Concrete\n",
        "    3: 3,     # Metal\n",
        "    4: 4,     # Road Marking\n",
        "    5: 5,     # Gravel\n",
        "    6: 6,     # Fabric\n",
        "    7: 7,     # Glass\n",
        "    8: 8,     # Plaster\n",
        "    9: 9,     # Plastic\n",
        "    10: 10,   # Rubber\n",
        "    11: 11,   # Sand\n",
        "    12: 12,   # Ceramic\n",
        "    13: 13,   # Cobblestone\n",
        "    14: 14,   # Brick\n",
        "    15: 15,   # Grass\n",
        "    16: 16,   # Wood\n",
        "    17: 17,   # Leaf\n",
        "    18: 18,   # Water\n",
        "    19: 19,   # Human Body\n",
        "    20: 20,   # Sky\n",
        "    255: 0     # Optional: Map 255 to Background\n",
        "}\n",
        "\n",
        "# Check if all grayscale values are mapped\n",
        "unmapped_grayscale = unique_grayscale - set(grayscale_to_class.keys())\n",
        "if len(unmapped_grayscale) > 0:\n",
        "    print(f\"\\nError: The following grayscale values are not mapped to any class: {sorted(unmapped_grayscale)}\")\n",
        "    print(\"Please update 'grayscale_to_class' to include these values.\")\n",
        "else:\n",
        "    print(\"\\nAll grayscale values are successfully mapped to class indices.\")\n",
        "\n",
        "# Optionally, save the mapping for future reference\n",
        "mapping_df = pd.DataFrame(list(grayscale_to_class.items()), columns=['Grayscale', 'Class_Index'])\n",
        "mapping_csv_path = \"/content/drive/MyDrive/multimodal_dataset/grayscale_to_class_mapping.csv\"\n",
        "mapping_df.to_csv(mapping_csv_path, index=False)\n",
        "print(f\"Grayscale to Class mapping saved to {mapping_csv_path}\")\n"
      ],
      "metadata": {
        "id": "Rj53dqnUWi-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Update Dataset Instances and DataLoaders with Complete Mapping\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'grayscale_to_class' has been defined in Cell 8 and includes all necessary mappings\n",
        "\n",
        "# Define data transformations\n",
        "# Define transformations for training\n",
        "train_transforms = A.Compose([\n",
        "    A.Resize(height=512, width=512),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "], additional_targets={'mask': 'mask'})\n",
        "\n",
        "# Define transformations for validation and testing\n",
        "val_transforms = A.Compose([\n",
        "    A.Resize(height=512, width=512),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "], additional_targets={'mask': 'mask'})\n",
        "\n",
        "# Update the grayscale_to_class mapping based on Cell 8\n",
        "# Ensure that the mapping correctly reflects your dataset's class definitions\n",
        "\n",
        "# Example:\n",
        "# grayscale_to_class = {\n",
        "#     0: 0,     # Background\n",
        "#     1: 1,     # Class 1\n",
        "#     2: 2,     # Class 2\n",
        "#     ...\n",
        "#     255: 20,  # Optional: Special label\n",
        "# }\n",
        "\n",
        "# Create Dataset instances with the updated mapping\n",
        "train_dataset = MaterialSegmentationDataset(\n",
        "    list_file=train_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=train_transforms,\n",
        "    grayscale_to_class=grayscale_to_class\n",
        ")\n",
        "\n",
        "val_dataset = MaterialSegmentationDataset(\n",
        "    list_file=val_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=val_transforms,\n",
        "    grayscale_to_class=grayscale_to_class\n",
        ")\n",
        "\n",
        "test_dataset = MaterialSegmentationDataset(\n",
        "    list_file=test_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=val_transforms,  # Typically, no augmentation for test\n",
        "    grayscale_to_class=grayscale_to_class\n",
        ")\n",
        "\n",
        "# Define DataLoaders\n",
        "batch_size = 8\n",
        "num_workers = 4  # Adjust based on your environment\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Verify Dataset Lengths\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
        "\n",
        "# Visualize a Batch from the Training Loader\n",
        "\n",
        "def visualize_batch(images, masks, batch_size=4):\n",
        "    images = images.permute(0, 2, 3, 1).cpu().numpy()\n",
        "    images = (images * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "    images = np.clip(images, 0, 1)\n",
        "\n",
        "    masks = masks.cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    for i in range(batch_size):\n",
        "        plt.subplot(2, batch_size, i+1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(\"Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, batch_size, batch_size + i + 1)\n",
        "        plt.imshow(masks[i], cmap='jet')\n",
        "        plt.title(\"Mask\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get a batch from the training loader\n",
        "for images, masks in train_loader:\n",
        "    visualize_batch(images, masks, batch_size=4)\n",
        "    break  # Only visualize one batch\n"
      ],
      "metadata": {
        "id": "OQDSnufhWrnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Update Dataset Instances and DataLoaders with Complete Mapping\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define data transformations\n",
        "# Define transformations for training\n",
        "train_transforms = A.Compose([\n",
        "    A.Resize(height=512, width=512),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "], additional_targets={'mask': 'mask'})\n",
        "\n",
        "# Define transformations for validation and testing\n",
        "val_transforms = A.Compose([\n",
        "    A.Resize(height=512, width=512),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "], additional_targets={'mask': 'mask'})\n",
        "\n",
        "# Ensure that 'grayscale_to_class' has been defined and includes all necessary mappings\n",
        "# Example:\n",
        "# grayscale_to_class = {\n",
        "#     0: 0,    # Background\n",
        "#     1: 1,    # Concrete\n",
        "#     2: 2,    # Asphalt\n",
        "#     3: 3,    # Grass\n",
        "#     4: 4,    # Water\n",
        "#     # Add other mappings based on your observations\n",
        "#     255: 20  # Optional: Special label\n",
        "# }\n",
        "\n",
        "# Create Dataset instances with the updated mapping\n",
        "train_dataset = MaterialSegmentationDataset(\n",
        "    list_file=train_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=train_transforms,\n",
        "    grayscale_to_class=grayscale_to_class\n",
        ")\n",
        "\n",
        "val_dataset = MaterialSegmentationDataset(\n",
        "    list_file=val_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=val_transforms,\n",
        "    grayscale_to_class=grayscale_to_class\n",
        ")\n",
        "\n",
        "test_dataset = MaterialSegmentationDataset(\n",
        "    list_file=test_list,\n",
        "    images_dirs=images_dirs,\n",
        "    masks_dirs=masks_dirs,\n",
        "    transforms=val_transforms,  # Typically, no augmentation for test\n",
        "    grayscale_to_class=grayscale_to_class\n",
        ")\n",
        "\n",
        "# Define DataLoaders\n",
        "batch_size = 8\n",
        "num_workers = 4  # Adjust based on your environment\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Verify Dataset Lengths\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
        "\n",
        "# Visualize a Batch from the Training Loader\n",
        "\n",
        "def visualize_batch(images, masks, batch_size=4):\n",
        "    images = images.permute(0, 2, 3, 1).cpu().numpy()\n",
        "    images = (images * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "    images = np.clip(images, 0, 1)\n",
        "\n",
        "    masks = masks.cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    for i in range(batch_size):\n",
        "        plt.subplot(2, batch_size, i+1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(\"Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, batch_size, batch_size + i + 1)\n",
        "        plt.imshow(masks[i], cmap='jet')\n",
        "        plt.title(\"Mask\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get a batch from the training loader\n",
        "for images, masks in train_loader:\n",
        "    visualize_batch(images, masks, batch_size=4)\n",
        "    break  # Only visualize one batch\n"
      ],
      "metadata": {
        "id": "NZtTm6-DWsMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Verify Dataset and DataLoaders\n",
        "\n",
        "num_classes = 13  # Updated to match the MCubeS dataset\n",
        "\n",
        "# Function to collect unique labels in masks\n",
        "def collect_unique_labels(dataset):\n",
        "    unique_values = set()\n",
        "    for idx in range(len(dataset)):\n",
        "        _, mask = dataset[idx]\n",
        "        unique_values.update(np.unique(mask.numpy()))\n",
        "    return unique_values\n",
        "\n",
        "# Collect unique labels from training dataset\n",
        "unique_values_train = collect_unique_labels(train_dataset)\n",
        "print(f\"Unique labels in training masks: {unique_values_train}\")\n",
        "\n",
        "# Collect unique labels from validation dataset\n",
        "unique_values_val = collect_unique_labels(val_dataset)\n",
        "print(f\"Unique labels in validation masks: {unique_values_val}\")\n",
        "\n",
        "# Collect unique labels from test dataset\n",
        "unique_values_test = collect_unique_labels(test_dataset)\n",
        "print(f\"Unique labels in test masks: {unique_values_test}\")\n",
        "\n",
        "# Check class distribution in training dataset\n",
        "def compute_class_distribution(dataset, num_classes):\n",
        "    class_counts = np.zeros(num_classes, dtype=np.int64)\n",
        "    for idx in range(len(dataset)):\n",
        "        _, mask = dataset[idx]\n",
        "        mask_np = mask.numpy()\n",
        "        for cls in range(num_classes):\n",
        "            class_counts[cls] += np.sum(mask_np == cls)\n",
        "    total_pixels = np.sum(class_counts)\n",
        "    for cls in range(num_classes):\n",
        "        percentage = (class_counts[cls] / total_pixels) * 100\n",
        "        print(f\"Class {cls}: {class_counts[cls]} pixels ({percentage:.2f}%)\")\n",
        "    return class_counts\n",
        "\n",
        "print(\"\\nClass distribution in training dataset:\")\n",
        "class_counts_train = compute_class_distribution(train_dataset, num_classes)\n",
        "\n",
        "# Optional: Visualize class distribution\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=list(range(num_classes)), y=class_counts_train)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Pixel Count')\n",
        "plt.title('Class Distribution in Training Dataset')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cBEcxOzlWwY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Define and Initialize the Segmentation Model\n",
        "\n",
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize DeepLabV3 with ResNet-50 backbone\n",
        "model = models.segmentation.deeplabv3_resnet50(weights='DEFAULT')\n",
        "\n",
        "# Modify the classifier to match the number of classes\n",
        "num_classes = 13  # Updated to match the MCubeS dataset\n",
        "\n",
        "# Replace the classifier with a new one (DeepLabHead)\n",
        "model.classifier = models.segmentation.deeplabv3.DeepLabHead(2048, num_classes)\n",
        "print(\"Model classifier modified successfully.\")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "print(\"Model moved to device successfully.\")\n"
      ],
      "metadata": {
        "id": "SaEFoe3TWzPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Load Grounding DINO and SAM Models\n",
        "\n",
        "import torch\n",
        "from groundingdino.models import build_model\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.utils import clean_state_dict\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "\n",
        "def load_grounding_dino(config_path, checkpoint_path, device='cuda'):\n",
        "    \"\"\"\n",
        "    Load the Grounding DINO model from configuration and checkpoint files.\n",
        "    \"\"\"\n",
        "    # Load configuration\n",
        "    cfg = SLConfig.fromfile(config_path)\n",
        "\n",
        "    # Build model\n",
        "    model = build_model(cfg)\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
        "\n",
        "    # Move model to device and set to evaluation mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_sam(sam_checkpoint_path, model_type=\"vit_b\", device='cuda'):\n",
        "    \"\"\"\n",
        "    Load the SAM model and initialize the predictor.\n",
        "    \"\"\"\n",
        "    # Register and load SAM model\n",
        "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint_path)\n",
        "    sam.to(device)\n",
        "    sam_predictor = SamPredictor(sam)\n",
        "\n",
        "    return sam_predictor\n",
        "\n",
        "def load_image(image_path):\n",
        "    \"\"\"\n",
        "    Load an image from the specified path and preprocess it.\n",
        "    \"\"\"\n",
        "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                             std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "    image_tensor = preprocess(image_pil)\n",
        "\n",
        "    return image_pil, image_tensor\n",
        "\n",
        "def get_grounding_output(model, image, caption, box_threshold, text_threshold, device='cuda'):\n",
        "    \"\"\"\n",
        "    Perform Grounding DINO inference to obtain bounding boxes and associated phrases.\n",
        "\n",
        "    Args:\n",
        "        model: The loaded Grounding DINO model.\n",
        "        image (torch.Tensor): Preprocessed image tensor of shape (C, H, W).\n",
        "        caption (str): Text prompt containing multiple categories separated by commas.\n",
        "        box_threshold (float): Confidence threshold to filter bounding boxes.\n",
        "        text_threshold (float): Confidence threshold for text recognition.\n",
        "        device (str): Device to perform computation ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        boxes_filt (torch.Tensor): Filtered bounding boxes with shape (num_boxes, 4).\n",
        "        pred_phrases (list): List of predicted phrases corresponding to each bounding box.\n",
        "        scores (torch.Tensor): Confidence scores for each bounding box.\n",
        "    \"\"\"\n",
        "    image = image.to(device)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image[None], text=[caption])\n",
        "\n",
        "    # Check if 'pred_logits' and 'pred_boxes' are in outputs\n",
        "    if 'pred_logits' not in outputs or 'pred_boxes' not in outputs:\n",
        "        print(\"Model outputs do not contain 'pred_logits' or 'pred_boxes'.\")\n",
        "        return torch.empty((0, 4)).to(device), [], torch.empty((0,)).to(device)\n",
        "\n",
        "    logits = outputs[\"pred_logits\"].sigmoid()[0]  # Shape: (num_queries, vocab_size)\n",
        "    boxes = outputs[\"pred_boxes\"][0]              # Shape: (num_queries, 4)\n",
        "\n",
        "    # Filter boxes with confidence threshold\n",
        "    logits_max, _ = logits.max(dim=1)\n",
        "    keep = logits_max > box_threshold\n",
        "    if keep.sum() == 0:\n",
        "        print(\"No boxes above box_threshold.\")\n",
        "        return torch.empty((0, 4)).to(device), [], torch.empty((0,)).to(device)\n",
        "    logits_filt = logits[keep]\n",
        "    boxes_filt = boxes[keep]\n",
        "\n",
        "    # Tokenize caption\n",
        "    tokenized = tokenizer(caption, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Map logits to phrases\n",
        "    pred_phrases = []\n",
        "    scores = []\n",
        "    for logit in logits_filt:\n",
        "        # Find tokens with confidence above text_threshold\n",
        "        token_indices = (logit > text_threshold).nonzero(as_tuple=True)[0]\n",
        "        if len(token_indices) == 0:\n",
        "            continue\n",
        "        tokens = tokenized[\"input_ids\"][0][token_indices]\n",
        "        phrase = tokenizer.decode(tokens)\n",
        "        pred_phrases.append(phrase)\n",
        "        scores.append(logit[token_indices].mean().item())\n",
        "\n",
        "    if len(scores) == 0:\n",
        "        print(\"No scores above text_threshold.\")\n",
        "        return torch.empty((0, 4)).to(device), [], torch.empty((0,)).to(device)\n",
        "\n",
        "    boxes_filt = boxes_filt[:len(pred_phrases)]\n",
        "    scores = torch.tensor(scores).to(device)\n",
        "    return boxes_filt, pred_phrases, scores\n",
        "\n",
        "def segment_with_sam(image_pil, boxes, predictor):\n",
        "    \"\"\"\n",
        "    Perform segmentation using the SAM model based on bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        image_pil (PIL.Image.Image): The input image.\n",
        "        boxes (torch.Tensor): Bounding boxes detected by Grounding DINO.\n",
        "        predictor: The SAM predictor object.\n",
        "\n",
        "    Returns:\n",
        "        List of NumPy arrays representing segmentation masks.\n",
        "    \"\"\"\n",
        "    if boxes.numel() == 0:\n",
        "        print(\"No boxes provided for segmentation.\")\n",
        "        return []\n",
        "\n",
        "    image_np = np.array(image_pil)\n",
        "    predictor.set_image(image_np)\n",
        "    masks = []\n",
        "\n",
        "    transformed_boxes = predictor.transform.apply_boxes_torch(boxes, image_np.shape[:2])\n",
        "\n",
        "    for box in transformed_boxes:\n",
        "        # Perform prediction\n",
        "        masks_pred, _, _ = predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=box.unsqueeze(0).to(device),\n",
        "            multimask_output=False,\n",
        "        )\n",
        "\n",
        "        # Convert mask to NumPy array\n",
        "        mask_np = masks_pred[0].cpu().numpy()\n",
        "        masks.append(mask_np)\n",
        "\n",
        "    return masks\n",
        "\n",
        "print(\"Helper functions loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "fCaeT0yJW2DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Upgrade `timm` to the Latest Version\n",
        "\n",
        "!pip install --upgrade timm\n",
        "\n",
        "print(\"Upgraded timm to the latest version.\")\n"
      ],
      "metadata": {
        "id": "pyXkrV3ZW44K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: Verify Helper Functions Defined Correctly\n",
        "\n",
        "try:\n",
        "    print(load_grounding_dino)\n",
        "    print(load_sam)\n",
        "    print(load_image)\n",
        "    print(get_grounding_output)\n",
        "    print(segment_with_sam)\n",
        "    print(\"All helper functions are defined correctly.\")\n",
        "except NameError as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "Ao69fIFLXIVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Define Loss Function, Optimizer, and Scheduler with Class Weights\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Compute class weights based on class counts in the training dataset\n",
        "all_masks = []\n",
        "for _, mask in train_dataset:\n",
        "    all_masks.extend(mask.numpy().flatten())\n",
        "all_masks = np.array(all_masks)\n",
        "\n",
        "# Get unique labels in all_masks\n",
        "unique_labels = np.unique(all_masks)\n",
        "print(f\"Unique labels in training masks: {unique_labels}\")\n",
        "\n",
        "# Define number of classes\n",
        "num_classes = 13  # Ensure this matches your dataset\n",
        "\n",
        "# Compute class weights using only the classes present in the training data\n",
        "class_weights_raw = compute_class_weight(class_weight='balanced',\n",
        "                                         classes=unique_labels,\n",
        "                                         y=all_masks)\n",
        "\n",
        "# Initialize class_weights array with ones\n",
        "class_weights = np.ones(num_classes, dtype=np.float32)\n",
        "\n",
        "# Assign the computed weights to the corresponding indices\n",
        "for i, cls in enumerate(unique_labels):\n",
        "    class_weights[int(cls)] = class_weights_raw[i]\n",
        "\n",
        "# Convert class_weights to a tensor and move to the appropriate device\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "print(f\"Class Weights: {class_weights}\")\n",
        "\n",
        "# Loss function with class weights\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Optimizer\n",
        "learning_rate = 1e-4\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
      ],
      "metadata": {
        "id": "6Ro8BhnXXKAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 17: Verify Mask Classes in All Datasets\n",
        "\n",
        "def verify_mask_classes(dataset, num_classes):\n",
        "    unique_classes = set()\n",
        "    for idx in range(len(dataset)):\n",
        "        _, mask = dataset[idx]\n",
        "        unique_classes.update(mask.numpy().flatten())\n",
        "    print(f\"Unique classes in the dataset: {sorted(unique_classes)}\")\n",
        "    missing_classes = set(range(num_classes)) - unique_classes\n",
        "    if missing_classes:\n",
        "        print(f\"Warning: The following classes are missing in the dataset: {missing_classes}\")\n",
        "    else:\n",
        "        print(\"All classes are present in the dataset.\")\n",
        "\n",
        "# Verify training dataset\n",
        "print(\"Verifying training dataset:\")\n",
        "verify_mask_classes(train_dataset, num_classes=13)\n",
        "\n",
        "# Verify validation dataset\n",
        "print(\"\\nVerifying validation dataset:\")\n",
        "verify_mask_classes(val_dataset, num_classes=13)\n",
        "\n",
        "# Verify test dataset\n",
        "print(\"\\nVerifying test dataset:\")\n",
        "verify_mask_classes(test_dataset, num_classes=13)\n"
      ],
      "metadata": {
        "id": "TSeQ1EnFXLnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 18: Visualize Sample Images and Masks\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_sample(dataset, idx):\n",
        "    image, mask = dataset[idx]\n",
        "    image_np = image.cpu().numpy().transpose(1, 2, 0)\n",
        "    image_np = (image_np * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "    image_np = np.clip(image_np, 0, 1)\n",
        "\n",
        "    mask_np = mask.cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image_np)\n",
        "    plt.title(\"Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask_np, cmap='jet')\n",
        "    plt.title(\"Mask\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Visualize first 3 samples from training dataset\n",
        "for i in range(3):\n",
        "    visualize_sample(train_dataset, i)\n"
      ],
      "metadata": {
        "id": "K4kh9MHgXPLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 19: Initialize the Segmentation Model\n",
        "\n",
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize DeepLabV3 with ResNet-50 backbone\n",
        "model = models.segmentation.deeplabv3_resnet50(weights='DEFAULT')\n",
        "\n",
        "# Modify the classifier to match the number of classes\n",
        "num_classes = 13  # Updated to match the MCubeS dataset\n",
        "\n",
        "# Replace the classifier with a new one (DeepLabHead)\n",
        "model.classifier = models.segmentation.deeplabv3.DeepLabHead(2048, num_classes)\n",
        "print(\"Model classifier modified successfully.\")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "print(\"Model moved to device successfully.\")\n"
      ],
      "metadata": {
        "id": "w-aGyJU2XQ9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 20: Define and Initialize the SAM Predictor\n",
        "\n",
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "import os\n",
        "\n",
        "def load_sam_predictor(sam_checkpoint_path, model_type=\"vit_b\", device='cuda'):\n",
        "    \"\"\"\n",
        "    Load the SAM model and initialize the predictor.\n",
        "    \"\"\"\n",
        "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint_path)\n",
        "    sam.to(device)\n",
        "    sam_predictor = SamPredictor(sam)\n",
        "    return sam_predictor\n",
        "\n",
        "# Define SAM checkpoint path\n",
        "sam_checkpoint_path = \"/content/segment-anything/checkpoints/sam_vit_b_01ec64.pth\"\n",
        "\n",
        "# Verify if the SAM checkpoint exists\n",
        "if not os.path.exists(sam_checkpoint_path):\n",
        "    print(f\"❌ SAM checkpoint file not found at {sam_checkpoint_path}. Please download it and place it in the specified directory.\")\n",
        "else:\n",
        "    # Load SAM predictor\n",
        "    sam_predictor = load_sam_predictor(\n",
        "        sam_checkpoint_path=sam_checkpoint_path,\n",
        "        model_type=\"vit_b\",\n",
        "        device=device\n",
        "    )\n",
        "    print(\"SAM predictor initialized successfully.\")\n"
      ],
      "metadata": {
        "id": "7SGfzwO4XU2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 21: Define the Training Loop\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch\n",
        "\n",
        "# Initialize TensorBoard writer\n",
        "writer = SummaryWriter(\"/content/drive/MyDrive/MaterialSegmentationOutput/tensorboard_logs\")\n",
        "\n",
        "num_epochs = 25\n",
        "patience = 5  # For early stopping\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# Initialize GradScaler for mixed precision training\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, masks in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(images)['out']\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    print(f\"Train Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device).long()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(images)['out']\n",
        "                loss = criterion(outputs, masks)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == masks).sum().item()\n",
        "            total += masks.numel()\n",
        "\n",
        "    epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
        "    epoch_val_accuracy = correct / total\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "    print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {epoch_val_accuracy:.4f}\")\n",
        "\n",
        "    # Log metrics to TensorBoard\n",
        "    writer.add_scalar('Train/Loss', epoch_train_loss, epoch+1)\n",
        "    writer.add_scalar('Validation/Loss', epoch_val_loss, epoch+1)\n",
        "    writer.add_scalar('Validation/Accuracy', epoch_val_accuracy, epoch+1)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        counter = 0\n",
        "        # Save the best model\n",
        "        best_model_path = \"/content/drive/MyDrive/MaterialSegmentationOutput/checkpoints/best_model.pth\"\n",
        "        os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved to {best_model_path}\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement in validation loss for {counter} epoch(s).\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # Save model checkpoint\n",
        "    checkpoint_path = f\"/content/drive/MyDrive/MaterialSegmentationOutput/checkpoints/model_epoch_{epoch + 1}.pth\"\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"Saved model checkpoint to {checkpoint_path}\\n\")\n",
        "\n",
        "print(\"Training completed.\")\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "sx4iFZSyXYUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 22: Save and Visualize Training Metrics\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Save metrics to CSV\n",
        "metrics = {\n",
        "    \"Epoch\": list(range(1, len(train_losses) + 1)),\n",
        "    \"Train_Loss\": train_losses,\n",
        "    \"Validation_Loss\": val_losses,\n",
        "    \"Validation_Accuracy\": val_accuracies\n",
        "}\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "metrics_csv_path = \"/content/drive/MyDrive/MaterialSegmentationOutput/training_metrics.csv\"\n",
        "df_metrics.to_csv(metrics_csv_path, index=False)\n",
        "print(f\"Training metrics saved to {metrics_csv_path}\")\n",
        "\n",
        "# Plot Losses\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(df_metrics['Epoch'], df_metrics['Train_Loss'], label='Train Loss')\n",
        "plt.plot(df_metrics['Epoch'], df_metrics['Validation_Loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(df_metrics['Epoch'], df_metrics['Validation_Accuracy'], label='Validation Accuracy', color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LC2h1cf0XgzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 23: Launch TensorBoard\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Launch TensorBoard\n",
        "%tensorboard --logdir /content/drive/MyDrive/MaterialSegmentationOutput/tensorboard_logs\n"
      ],
      "metadata": {
        "id": "TskDTU6VXitk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 24: Evaluate the Model on the Test Set\n",
        "\n",
        "def calculate_iou(pred, target, num_classes):\n",
        "    ious = []\n",
        "    pred = pred.flatten()\n",
        "    target = target.flatten()\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = pred == cls\n",
        "        target_inds = target == cls\n",
        "        intersection = (pred_inds & target_inds).sum()\n",
        "        union = (pred_inds | target_inds).sum()\n",
        "        if union == 0:\n",
        "            ious.append(float('nan'))  # If no ground truth, do not include in evaluation\n",
        "        else:\n",
        "            ious.append(intersection / union)\n",
        "    return ious\n",
        "\n",
        "def evaluate(model, dataloader, device, num_classes):\n",
        "    model.eval()\n",
        "    iou_scores = []\n",
        "    accuracy_total = 0\n",
        "    total_pixels = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device).long()\n",
        "\n",
        "            outputs = model(images)['out']\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # Calculate pixel-wise accuracy\n",
        "            correct = (preds == masks).sum().item()\n",
        "            total = masks.numel()\n",
        "            accuracy_total += correct\n",
        "            total_pixels += total\n",
        "\n",
        "            for pred, mask in zip(preds, masks):\n",
        "                iou = calculate_iou(pred.cpu().numpy(), mask.cpu().numpy(), num_classes)\n",
        "                iou_scores.append(iou)\n",
        "\n",
        "    accuracy = accuracy_total / total_pixels\n",
        "    # Calculate mean IoU for each class\n",
        "    iou_scores = np.array(iou_scores)\n",
        "    mean_ious = np.nanmean(iou_scores, axis=0)\n",
        "    for cls_idx, mean_iou in enumerate(mean_ious):\n",
        "        print(f\"Class {cls_idx}: Mean IoU = {mean_iou:.4f}\")\n",
        "    # Overall Mean IoU\n",
        "    overall_mean_iou = np.nanmean(mean_ious)\n",
        "    print(f\"Overall Mean IoU: {overall_mean_iou:.4f}\")\n",
        "    print(f\"Pixel-wise Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Define number of classes\n",
        "num_classes = 13  # Ensure this matches your dataset\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate(model, test_loader, device, num_classes)\n"
      ],
      "metadata": {
        "id": "RmgmOv-BXkV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 25: Visualize Model Predictions on Test Set\n",
        "\n",
        "def visualize_test_predictions(model, dataset, device, num_samples=5):\n",
        "    model.eval()\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "\n",
        "    for idx in indices:\n",
        "        image, mask = dataset[idx]\n",
        "        image_input = image.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image_input)['out'][0]\n",
        "            pred_mask = torch.argmax(output, dim=0).cpu().numpy()\n",
        "\n",
        "        # Convert tensors to NumPy arrays for visualization\n",
        "        image_np = image.cpu().numpy().transpose(1, 2, 0)\n",
        "        image_np = (image_np * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "        image_np = np.clip(image_np, 0, 1)\n",
        "\n",
        "        mask_np = mask.cpu().numpy()\n",
        "\n",
        "        # Create color masks\n",
        "        color_mask = np.zeros_like(image_np)\n",
        "        pred_color_mask = np.zeros_like(image_np)\n",
        "\n",
        "        for cls in range(num_classes):\n",
        "            if cls in CATEGORY_COLORS:\n",
        "                color = CATEGORY_COLORS[cls]\n",
        "                color_mask[mask_np == cls] = np.array(color) / 255.0\n",
        "                pred_color_mask[pred_mask == cls] = np.array(color) / 255.0\n",
        "\n",
        "        # Overlay masks on the image\n",
        "        overlay_true = (0.5 * image_np + 0.5 * color_mask)\n",
        "        overlay_pred = (0.5 * image_np + 0.5 * pred_color_mask)\n",
        "\n",
        "        # Plotting\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "        axs[0].imshow(image_np)\n",
        "        axs[0].set_title(\"Original Image\")\n",
        "        axs[0].axis('off')\n",
        "\n",
        "        axs[1].imshow(mask_np, cmap='jet', alpha=0.5)\n",
        "        axs[1].set_title(\"Ground Truth Mask\")\n",
        "        axs[1].axis('off')\n",
        "\n",
        "        axs[2].imshow(pred_mask, cmap='jet', alpha=0.5)\n",
        "        axs[2].set_title(\"Predicted Mask\")\n",
        "        axs[2].axis('off')\n",
        "\n",
        "        axs[3].imshow(overlay_pred)\n",
        "        axs[3].set_title(\"Overlay Predicted Mask\")\n",
        "        axs[3].axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# Define CATEGORY_COLORS for visualization\n",
        "CATEGORY_COLORS = {\n",
        "    0: (0, 0, 0),         # Background\n",
        "    1: (128, 0, 0),       # Class 1\n",
        "    2: (0, 128, 0),       # Class 2\n",
        "    3: (128, 128, 0),     # Class 3\n",
        "    4: (0, 0, 128),       # Class 4\n",
        "    5: (128, 0, 128),     # Class 5\n",
        "    6: (0, 128, 128),     # Class 6\n",
        "    7: (128, 128, 128),   # Class 7\n",
        "    8: (64, 0, 0),        # Class 8\n",
        "    9: (192, 0, 0),       # Class 9\n",
        "    10: (64, 128, 0),     # Class 10\n",
        "    11: (192, 128, 0),    # Class 11\n",
        "    12: (64, 0, 128),     # Class 12\n",
        "}\n",
        "\n",
        "# Visualize predictions on test dataset\n",
        "visualize_test_predictions(model, test_dataset, device, num_samples=5)\n"
      ],
      "metadata": {
        "id": "_7Q_p7mBXk9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 26: Shutdown TensorBoard (Optional)\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "dxTr7GQZXoS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RpJYVWn0Xqni"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}